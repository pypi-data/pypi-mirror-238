{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDUtcLEmtIlk"
      },
      "outputs": [],
      "source": [
        "%pip install langkit[all]==0.0.23.dev0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqPATL2OvFAG"
      },
      "source": [
        "Let's say we want the standard LLM metrics for English prompts, but we don't want any metrics on the responses since they're in a different unsupported language. We can use the default configurations for the prompt metrics and set the response metric configurations to `None` to turn them off.\n",
        "\n",
        "*Leaves some text stat metrics on response column*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U_TM-3Xtx-Z"
      },
      "outputs": [],
      "source": [
        "import langkit.llm_metrics as llmm\n",
        "from langkit import LangKitConfig\n",
        "\n",
        "no_response_config = LangKitConfig(\n",
        "    response_pattern_file_path=None,  # regexes are semi-international, could leave this as default\n",
        "    response_sentiment_lexicon=None,\n",
        "    response_theme_file_path=None,\n",
        "    response_topic_model_path=None,\n",
        "    response_toxicity_model_path=None,\n",
        "    response_transformer_name=None,\n",
        ")\n",
        "schema = llmm.init(config=no_response_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQDvPLvR8E_U",
        "outputId": "41b76313-20c5-487f-f914-785f9491fd27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt\n",
            "response\n",
            "prompt.has_patterns\n",
            "prompt.sentiment_nltk\n",
            "prompt.flesch_reading_ease\n",
            "response.flesch_reading_ease\n",
            "prompt.automated_readability_index\n",
            "response.automated_readability_index\n",
            "prompt.aggregate_reading_level\n",
            "response.aggregate_reading_level\n",
            "prompt.syllable_count\n",
            "response.syllable_count\n",
            "prompt.lexicon_count\n",
            "response.lexicon_count\n",
            "prompt.sentence_count\n",
            "response.sentence_count\n",
            "prompt.character_count\n",
            "response.character_count\n",
            "prompt.letter_count\n",
            "response.letter_count\n",
            "prompt.polysyllable_count\n",
            "response.polysyllable_count\n",
            "prompt.monosyllable_count\n",
            "response.monosyllable_count\n",
            "prompt.difficult_words\n",
            "response.difficult_words\n",
            "prompt.jailbreak_similarity\n",
            "prompt.toxicity\n",
            "response.relevance_to_prompt\n"
          ]
        }
      ],
      "source": [
        "import whylogs as why\n",
        "from langkit.whylogs.samples import load_chats, show_first_chat\n",
        "\n",
        "chats = load_chats()\n",
        "results = why.log(chats, schema=schema)\n",
        "\n",
        "for column in results.view().get_columns().keys():\n",
        "  print(column)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
