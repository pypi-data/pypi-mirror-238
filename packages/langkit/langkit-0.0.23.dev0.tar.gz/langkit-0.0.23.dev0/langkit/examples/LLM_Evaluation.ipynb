{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### üö© *Create a free WhyLabs account to complete this example!*<br> \n",
    ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=github&utm_medium=referral&utm_campaign=LLM_to_WhyLabs) to leverage the power of whylogs and WhyLabs together!*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Profiles to WhyLabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/LanguageToolkit/blob/main/langkit/examples/LLM_to_WhyLabs.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll show you how to evaluate an LLM with LangKit.\n",
    "We will:\n",
    "\n",
    "- Define environment variables with the appropriate Credentials and IDs\n",
    "- Log LLM prompts and responses into a profile\n",
    "- Use a whylogs telemetry agent to gather statistics on the prompts/response and send these to WhyLabs\n",
    "- View the systematic telemetry on your LLM in WhyLabs.\n",
    "\n",
    "> Don't want to bother with setting up your credentials or running live LLM interactions? We've already done it for you and uploaded LangKit telemetry from prompt/response LLM interactions to a public guest session in WhyLabs, no login required you can just click [here](https://hub.whylabsapp.com/resources/demo-llm-chatbot/columns/prompt.sentiment_nltk?dateRange=2023-06-08-to-2023-06-09&sortModelBy=LatestAlert&sortModelDirection=DESC&targetOrgId=demo&sessionToken=session-8gcsnbVy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing LangKit\n",
    "\n",
    "First, let's install __langkit__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "#%pip install langkit[all] -q\n",
    "%pip install -e ~/projects/v1/TextMetricsToolkit\n",
    "%pip install ragas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ~/projects/v1/whylogs/python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Setting the Environment Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to send our profile to WhyLabs, let's first set up an account. You can skip this if you already have an account and a model set up.\n",
    "\n",
    "We will need three pieces of information:\n",
    "\n",
    "- API tokens for the LLM and WhyLabs\n",
    "- Organization ID for WhyLabs\n",
    "- Dataset ID for WhyLabs\n",
    "\n",
    "Go to [https://whylabs.ai/free](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit) and grab a free account. You can follow along with the examples if you wish, but if you‚Äôre interested in only following this demonstration, you can go ahead and skip the quick start instructions.\n",
    "\n",
    "After that, you‚Äôll be prompted to create an API token. Once you create it, copy and store it locally. The second important information here is your org ID. Take note of it as well. After you get your API Token and Org ID, you can go to https://hub.whylabsapp.com/models to see your projects dashboard. You can create a new project and take note of it's ID (if it's a model project it will look like `model-xxxx`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now check if the required credentials are set as environment variables. In a production setting these would already be set as environment variables, but here we prompt you if they are missing. You can still run the example without these, but we won't use a live session with GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhyLabs Org ID is already set in env var to: org-e2qTar\n",
      "WhyLabs Dataset ID is already set in env var to: model-100\n",
      "Whylabs API Key already set with ID:  RdN37nDEdn\n",
      "OPENAI_API_KEY already set in env var, good job!\n"
     ]
    }
   ],
   "source": [
    "from langkit.config import check_or_prompt_for_api_keys\n",
    "from langkit.openai import ChatLog, Conversation, LLMInvocationParams, OpenAIDavinci, OpenAIDefault\n",
    "\n",
    "check_or_prompt_for_api_keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Profiling the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, let's use some archived chat gpt response/prompts data from Hugging Face, or you can set the interactive parameter to true and interact with ChatGPT to see how it works in real time if you already have an openai api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'text-davinci-003', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n",
      "{'model': 'gpt-3.5-turbo', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n",
      "{'model': 'gpt-4', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "from langkit.openai.openai import OpenAIGPT4, OpenAIDavinci\n",
    "\n",
    "\n",
    "llm0 = Conversation(invocation_params=OpenAIDavinci())\n",
    "llm1 = Conversation(invocation_params=OpenAIDefault())\n",
    "llm2 = Conversation(invocation_params=OpenAIGPT4())\n",
    "\n",
    "print(asdict(llm0.invocation_params))\n",
    "print(asdict(llm1.invocation_params))\n",
    "print(asdict(llm2.invocation_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jamie/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "‚ö†Ô∏è Initializing default session because no session was found.\n",
      "Initializing session with config /home/jamie/.config/whylogs/config.ini\n",
      "\n",
      "‚úÖ Using session type: LOCAL. Profiles won't be uploaded or written anywhere automatically.\n",
      "At any time input 'q' or anything that begins with q to quit. enter a question for the LLM\n",
      "\n",
      "<class 'str'>\n",
      "params are: {'model': 'gpt-3.5-turbo', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n",
      "text-davinci-003 response: {'prompt': 'who was the first US president?', 'response': '\\nThe first US president was George Washington, who served from 1789 to 1797.', 'errors': None}\n",
      "gpt-3.5-turbo response: {'prompt': 'who was the first US president?', 'response': 'The first president of the United States was George Washington.', 'errors': None}\n",
      "\n",
      "<class 'str'>\n",
      "params are: {'model': 'gpt-3.5-turbo', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trace_id was specified as None but there is already a trace_id defined in metadata[whylabs.traceId]: c5a18044-4a02-47ae-9197-76b11ae08e84\n",
      "trace_id was specified as None but there is already a trace_id defined in metadata[whylabs.traceId]: c9e26292-32d9-496a-831d-94686de17f25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-davinci-003 response: {'prompt': 'how old was he when he left office?', 'response': '\\nGeorge Washington was 67 years old when he left office in 1797.', 'errors': None}\n",
      "gpt-3.5-turbo response: {'prompt': 'how old was he when he left office?', 'response': 'George Washington left office as the first president of the United States at the age of 65.', 'errors': None}\n",
      "\n",
      "<class 'str'>\n",
      "params are: {'model': 'gpt-3.5-turbo', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trace_id was specified as None but there is already a trace_id defined in metadata[whylabs.traceId]: c5a18044-4a02-47ae-9197-76b11ae08e84\n",
      "trace_id was specified as None but there is already a trace_id defined in metadata[whylabs.traceId]: c9e26292-32d9-496a-831d-94686de17f25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-davinci-003 response: {'prompt': 'when was he born?', 'response': '\\nGeorge Washington was born on February 22, 1732.', 'errors': None}\n",
      "gpt-3.5-turbo response: {'prompt': 'when was he born?', 'response': 'George Washington was born on February 22, 1732.', 'errors': None}\n",
      "\n",
      "<class 'str'>\n",
      "params are: {'model': 'gpt-3.5-turbo', 'temperature': 0.9, 'max_tokens': 1024, 'frequency_penalty': 0, 'presence_penalty': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trace_id was specified as None but there is already a trace_id defined in metadata[whylabs.traceId]: c5a18044-4a02-47ae-9197-76b11ae08e84\n",
      "trace_id was specified as None but there is already a trace_id defined in metadata[whylabs.traceId]: c9e26292-32d9-496a-831d-94686de17f25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-davinci-003 response: {'prompt': 'how old is a person in the year 1797 who was born in 1732?', 'response': '\\nA person born in 1732 and living in 1797 would have been 65 years old.', 'errors': None}\n",
      "gpt-3.5-turbo response: {'prompt': 'how old is a person in the year 1797 who was born in 1732?', 'response': 'In the year 1797, a person born in 1732 would be 65 years old.', 'errors': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langkit.whylogs.rolling_logger import RollingLogger\n",
    "telemetry_agent0 = RollingLogger(dataset_id=\"model-102\")\n",
    "telemetry_agent1 = RollingLogger(dataset_id=\"model-103\")\n",
    "\n",
    "print(f\"At any time input 'q' or anything that begins with q to quit. enter a question for the LLM\")\n",
    "while True:\n",
    "    print()\n",
    "    interactive_prompt = input(\"ask chat gpt: \")\n",
    "    if interactive_prompt.startswith('q'):\n",
    "        break\n",
    "    response0 = llm0.send_prompt(interactive_prompt)\n",
    "    response1 = llm1.send_prompt(interactive_prompt)\n",
    "\n",
    "    # use the agent to log a dictionary, these should be flat\n",
    "    # to get the best results, in this case we log the prompt and response text\n",
    "    telemetry_agent0.log(response0.to_dict())\n",
    "    telemetry_agent1.log(response1.to_dict())\n",
    "    print(f\"{llm0.invocation_params.model} response: {response0.to_dict()}\", flush=True)\n",
    "    print(f\"{llm1.invocation_params.model} response: {response1.to_dict()}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice you can use context manager lifecycle events to automatically close\n",
    "# loggers, this helps trigger a write ahead of schedule to avoid truncating the last interval\n",
    "# data seen by the agent.\n",
    "telemetry_agent0.close()\n",
    "telemetry_agent1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ragas import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load in a wiki question and answer dataset\n",
    "dataset = load_dataset(\"explodinggradients/ragas-wikiqa\")\n",
    "\n",
    "# Hugging Face dataset, lets see what the keys and structure\n",
    "# look like\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset = []\n",
    "for row in dataset['train']:\n",
    "    local_dataset.append({\"question\": row['question'], \"ground_truth\": row['correct_answer']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in local_dataset:\n",
    "    llm0 = Conversation(invocation_params=OpenAIDavinci())\n",
    "    llm1 = Conversation(invocation_params=OpenAIDefault())\n",
    "    interactive_prompt = row['question']\n",
    "    response0 = llm0.send_prompt(interactive_prompt)\n",
    "    response1 = llm1.send_prompt(interactive_prompt)\n",
    "    row[\"llm0_answer\"] = response0.to_dict()['response']\n",
    "    row[\"llm1_answer\"] = response1.to_dict()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'ground_truth', 'llm0_answer', 'llm1_answer'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = local_dataset[0].keys()\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(local_dataset)):\n",
    "    gt = local_dataset[i]['ground_truth']\n",
    "    if isinstance(gt, str):\n",
    "        local_dataset[i]['ground_truth'] = [gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'ground_truth', 'llm0_answer', 'llm1_answer', 'contexts'])\n"
     ]
    }
   ],
   "source": [
    "keys = local_dataset[0].keys()\n",
    "transformed_dataset = {\n",
    "    key: [row[key] for row in local_dataset] for key in keys\n",
    "}\n",
    "original_dataset = dataset['train']\n",
    "contexts = [row['context'] for row in original_dataset]\n",
    "transformed_dataset['contexts'] = contexts\n",
    "print(transformed_dataset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'ground_truth', 'llm0_answer', 'llm1_answer', 'contexts'],\n",
       "    num_rows: 232\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "eval_dataset: Dataset = Dataset.from_dict(transformed_dataset)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm0_column_mapping = {\n",
    "    'question': 'question',\n",
    "    'contexts':'contexts',\n",
    "    'ground_truths':'ground_truth',\n",
    "    'answer': 'llm0_answer'\n",
    "}\n",
    "\n",
    "llm1_column_mapping = {\n",
    "    'question': 'question',\n",
    "    'contexts':'contexts',\n",
    "    'ground_truths':'ground_truth',\n",
    "    'answer': 'llm1_answer'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e78ea469234870a4aaef6475c44dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6563683cb3047eea2dfa8ed9e77d276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/57.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066201312d2845d793b873ebf09e4dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/517 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a36cc1d4e674c36805ff842e422483e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a49af7448d4498783b51a1664521b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:41<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      "  6%|‚ñã         | 1/16 [02:09<32:24, 129.66s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 12%|‚ñà‚ñé        | 2/16 [03:56<27:07, 116.23s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 19%|‚ñà‚ñâ        | 3/16 [06:10<26:57, 124.44s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 25%|‚ñà‚ñà‚ñå       | 4/16 [07:49<22:53, 114.48s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [09:11<18:50, 102.75s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [10:54<17:05, 102.56s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [13:33<18:10, 121.13s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [15:28<15:52, 119.09s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [18:58<17:14, 147.76s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [21:49<15:28, 154.81s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [23:41<11:48, 141.70s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [25:25<08:40, 130.13s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [26:44<05:44, 114.78s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [28:20<03:38, 109.01s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [29:37<01:39, 99.37s/it] WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [30:24<00:00, 114.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [29:00<00:00, 108.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [17:58<00:00, 67.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.3111, 'answer_relevancy': 0.8885, 'context_relevancy': 0.1115, 'faithfulness': 0.7094, 'context_recall': 0.7418}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm0_ragas_results = evaluate(eval_dataset, column_map=llm0_column_mapping) # evaluate is from ragas\n",
    "llm0_ragas_results # this {'ragas_score': 0.3111, 'answer_relevancy': 0.8885, 'context_relevancy': 0.1115, 'faithfulness': 0.7094, 'context_recall': 0.7418}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [04:47<00:00, 17.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      "  6%|‚ñã         | 1/16 [02:03<30:46, 123.10s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 12%|‚ñà‚ñé        | 2/16 [03:38<24:58, 107.02s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 19%|‚ñà‚ñâ        | 3/16 [05:46<25:13, 116.44s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 25%|‚ñà‚ñà‚ñå       | 4/16 [07:52<24:02, 120.18s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [09:18<19:47, 107.99s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [10:56<17:23, 104.37s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [13:11<17:09, 114.44s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [14:33<13:54, 104.28s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [16:42<13:03, 111.87s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [19:15<12:27, 124.63s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [20:54<09:43, 116.71s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [22:38<07:31, 112.87s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [24:07<05:17, 105.71s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [25:47<03:27, 103.94s/it]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [26:58<01:34, 94.05s/it] WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [27:42<00:00, 103.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [21:36<12:45, 127.56s/it]WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [36:02<17:16, 259.20s/it]WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [53:24<00:00, 200.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [19:43<00:00, 73.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.3140, 'answer_relevancy': 0.8917, 'context_relevancy': 0.1115, 'faithfulness': 0.7705, 'context_recall': 0.7391}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm1_ragas_results = evaluate(eval_dataset, column_map=llm1_column_mapping) # evaluate is from ragas\n",
    "llm1_ragas_results # this {'ragas_score': 0.3140, 'answer_relevancy': 0.8917, 'context_relevancy': 0.1115, 'faithfulness': 0.7705, 'context_recall': 0.7391}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac4a3e83a4a4e0db985b3a8d5723214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_directory_name = \"eval_dataset\"\n",
    "eval_dataset.save_to_disk(eval_directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "eval_directory_name = \"eval_dataset\"\n",
    "\n",
    "# Load the dataset from the directory\n",
    "eval_dataset = load_from_disk(eval_directory_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d39f874c9b8a97550ecbd783714b95e79c9b905449b34f44c40e3bf053b54b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
