# -*- coding: utf-8 -*-t
"""
Created on Sun Jun 14 16:25:42 2020
Version 28 October 2023

@author: claudio
"""

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
from qutip import random_objects
from scipy.linalg import expm, sinm, cosm

tf_real = tf.float32
np_real = np.float32
tf_complex = tf.complex
np_complex = complex
default_N = 10


def RQRP(N=default_N, dtype=np_real, **kwargs):
    """

     Parameters
     ----------
     N : TYPE, dimension of R (2 times the number of bodies n)
         DESCRIPTION. The default is 10.

    Default type for the output matrices is np_real

     Returns
     -------
     The projection matricex RX and RP, and the matrix J

    """
    assert N % 2 == 0, " Dimension N must be even "
    n = np.floor_divide(N, 2)
    RQ = np.zeros((N, n), dtype=dtype)
    RP = np.zeros((N, n), dtype=dtype)
    c = -1
    d = -1
    for j in range(N):
        if j % 2 == 0:
            c = c + 1
            RQ[j, c] = 1.0
        else:
            d = d + 1
            RP[j, d] = 1.0
    J = np.matmul(RQ, RP.transpose()) - np.matmul(RP, RQ.transpose())
    return RQ, RP, J


def RandomSymplectic(N=default_N, **kwargs):
    """Return Random Square Symplectic M_np and its inverse MI_np
    generated by a Random Complex Unitary U_np with dimension N/2

    here M is symplectic with respect to J

    type can also be give at input as, e.g.,
    Examples
    --------
    RandomSymplectic(N, dtype=np.float32)

    Parameters
    ----------
    param: N size of output

    Returns
    -------
    M_np, MI_np: random symplectic matrix and invese
    U: associated unitary operator U
    """
    assert N % 2 == 0, "N must be even"
    n = np.floor_divide(N, 2)  # n = N/2
    wr = np.random.random((n, n))
    wi = np.random.random((n, n))
    # generate symmetric matrix
    HR = wr + wr.transpose()
    # generate an antisymmetric matrix
    HI = wi - wi.transpose()
    # exponentiate the Hermitian matrix
    # times the imaginary unit to have a unitary operator
    U = expm(-HI + 1j * HR)
    # return the real and imaginary part
    UR = np.real(U)
    UI = np.imag(U)
    RQ, RP, J = RQRP(N, **kwargs)
    M = (
        np.matmul(RQ, np.matmul(UR, RQ.transpose()))
        + np.matmul(RP, np.matmul(UR, RP.transpose()))
        - np.matmul(RQ, np.matmul(UI, RP.transpose()))
        + np.matmul(RP, np.matmul(UI, RQ.transpose()))
    )
    MI = np.matmul(np.transpose(np.matmul(M, J)), J)
    return M, MI, U


class GaussianLayer(layers.Layer):
    """
    Define a Gaussian state as a neural network layer

    When creating the object requires as input the g matrix, and optionally d

    Examples
    --------
    GaussianLayers(np.eye(N))
    GaussianLayers(np.eye(N),np.ones((N,1)))

    N is the size of the state vector x (must be even)

    n=N/2 is the number of bodies

    By default g and d are trainable

    Requires in the call an input with concatenated x
    and inverse of M for pullback
    """

    def __init__(
        self,
        g_in=np.eye(default_N, dtype=np_real),
        d_in=np.zeros((default_N, 1), dtype=np_real),
        trainable=True,
        **kwargs,
    ):
        super(GaussianLayer, self).__init__(**kwargs)
        self.trainable = trainable
        assert g_in.shape[0] == g_in.shape[1], "GaussianLayer, g is square"
        self.N = g_in.shape[0]
        assert self.N % 2 == 0, " Dimension N must be even "
        assert d_in.shape[0] == self.N, "GaussianLayer, d rows as g"
        self.g_np = g_in
        self.d_np = d_in
        with tf.name_scope(self.name) as scope:
            self.g = tf.Variable(
                self.g_np, dtype=self.dtype, trainable=self.trainable, name="g"
            )
            self.d = tf.Variable(
                self.d_np, dtype=self.dtype, trainable=self.trainable, name="d"
            )
        # define an internal layer for output scalar product
        self.DotLayer = tf.keras.layers.Dot(axes=1, dtype=self.dtype)

    def call(self, x, ain=None):
        # [chir, chii] = self.call(x,a)
        # [chir, chii] = self.call(x) , here a=0 as default
        # param: x the input row vector (Nbatch, n)
        # param: a the input ancillas column vector (n,1),
        # can be absent, if absent is assumed to be zero
        # output: chir, the real part of the characteristic function
        # output: chii, the imaginary part of the characteritic function
        if ain is None:
            a = tf.constant(np.zeros((self.N, 1)), dtype=self.dtype)
        else:
            a = ain
        yi = tf.matmul(x, self.d + a)
        yR = tf.matmul(
            x, self.g, transpose_b=True
        )  # note the transpose here, needed for DotLayer
        gDot = self.DotLayer([x, yR])
        yr2 = tf.multiply(tf.constant(-0.25, dtype=self.dtype), gDot)
        return [
            tf.multiply(tf.exp(yr2), tf.cos(yi)),
            tf.multiply(tf.exp(yr2), tf.sin(yi)),
        ]


def VacuumLayer(N=default_N, **kwargs):
    """Return a class ResidualGaussianMultihead non-trainable
    with identity matrix and zero displacement

    Parameters
    ----------
    N: size of the vector

    Returns
    -------
    ResidualGaussianMultiHead layer with
    g=np.eye(N) and d=np.zeros(N,1) trainable=False
    """

    return GaussianLayer(np.eye(N), np.zeros((N, 1)), trainable=False, **kwargs)


class LinearLayerConstant(layers.Layer):
    """
    Define a multi-headed linear layer corresponding to the
    \tilde a = U a
    with U a unitary complex matrix
    In the R space, corresponds to
    \tilde R = M R +d1

    In input has input N, the size of the R vector (must be even)
    RETURN y and MI inverse of M

    Examples
    --------
    L=LinearLayerConstant(np.eye(10),np.zeros(10,1),tf.double) # create class
    y, b = L(x) # zero ancillas
    y, b = L(x,a)

    Remark
    ------
    DOES NOT CHECK IF M IS SYMPLECTIC
    Dimension N is extracted from M, d1 dimension is checked for consistency

    TODO
    ----
    TODO, add control on the input M if symplectic
    """

    def __init__(self, M_np, d1_np, **kwargs):
        """init the linear layer

        Parameters
        ----------
        M_np : NxN symplectic matrix
        d1_np : Nx1 displacement vector


        Returns
        -------
        L : an object of the class LinearLayerConstant


        """
        super(LinearLayerConstant, self).__init__(**kwargs)
        N, Nc = M_np.shape
        Ndc, Ndr = d1_np.shape
        # check parameter size
        assert N % 2 == 0, " Dimension must be even "
        assert N == Nc, " M matrix must be squared "
        assert Ndc == N, " d1 must have dimensions Nx1 "
        assert Ndr == 1, " d1 must have dimensions Nx1 "
        self.N = N
        # Create the symplectic matrix
        _, _, J = RQRP(N)
        # Compute the inverse matrix
        MI_np = np.matmul(J.transpose(), np.matmul(M_np.transpose(), J))

        # store M and d as constant tensors
        self.M = tf.constant(M_np, dtype=self.dtype, name="M")
        self.d1 = tf.constant(d1_np, dtype=self.dtype, name="d")

        # inverse of M as constant tensors
        self.MI = tf.constant(MI_np, dtype=self.dtype)

    def call(self, x, a=None):
        """call the linear layer

        Parameters
        ----------
        x: input vector 1xN
        a: ancillas vectors Nx1 (if None is zero)

        Returns
        -------
        y = x M
        b = M^(-1) d1

        """
        y = tf.matmul(x, self.M)
        if a is None:
            b = tf.matmul(self.MI, self.d1)
        else:
            b = tf.matmul(self.MI, self.d1 + a)
        return [y, b]


def DisplacementLayerConstant(dtarget_np, **kwargs):
    """Return a constant displacemente layer

    Return LinearLayerConstant non-trainable with identity matrix
    and given input displacement

    Parameters
    ---------
    dtarget: displacement vector with shape (N,1)

    Returns
    -------
    output: LinearConstant M=np.eye(N) and d=dtarget
    """
    N = dtarget_np.shape[0]  # extract N from input
    assert dtarget_np.shape[-1] == 1, " Dtarget must be a column vector"
    L = LinearLayerConstant(np.eye(N), dtarget_np, **kwargs)
    return L


class RandomLayer(layers.Layer):
    """Define a linear random layer corresponding to the
    a = U a
    with U a unitary complex matrix
    In the R space, corresponds to
    x = R x
    In input has input N, the size of the R vector (must be even)

    Parameters in the constructor
    ----------------------------
    :param N: dimension
    :param trainable_M: if false M is not trained, default is true
    :param M_np: covariance matrix of the Gaussian layer
    :param d_np: displacement vector of the Gaussian layer
    (None is default for a zero non trainable d)
    :param trainable_d: if False the Gaussian covariance matrix and displacement are not trainable (defaul is true)

    :output x M
    :output b = M^(-1) d

    """

    def __init__(self, N=10, trainable_M=True, d_np=None, trainable_d=True, **kwargs):
        super(RandomLayer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension must be even "
        self.trainable_d = trainable_d
        self.trainable_M = trainable_M
        self.N = N
        n = np.floor_divide(self.N, 2)
        wr_np = np.random.random((n, n))
        wi_np = np.random.random((n, n))
        # define the name as scope (useful for plotting the model)
        with tf.name_scope(self.name) as scope:
            self.WR = tf.Variable(
                wr_np, dtype=self.dtype, trainable=self.trainable_M, name="WR"
            )
            self.WI = tf.Variable(
                wi_np, dtype=self.dtype, trainable=self.trainable_M, name="WI"
            )
        Rx, Rp, J = RQRP(N)
        self.Rx = tf.constant(Rx, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        self.J = tf.constant(J, dtype=self.dtype)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # the layer has a trainable displacement
            # if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)

    def get_M(self):
        """return the M matrix and its inverse MI"""
        # generate symmetric matrix
        HR = self.WR + tf.transpose(self.WR)
        # generate an antisymmetric matrix
        HI = self.WI - tf.transpose(self.WI)
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # return the real and immaginary part
        UR = tf.math.real(U)
        UI = tf.math.imag(U)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI

    def call(self, x, di=None):
        # generate symmetric matrix
        HR = self.WR + tf.transpose(self.WR)
        # generate an antisymmetric matrix
        HI = self.WI - tf.transpose(self.WI)
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # return the real and immaginary part
        UR = tf.math.real(U)
        UI = tf.math.imag(U)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        # no transpose here for M, as M is already transpose
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        # return tensor list(check this ... and test x,y =R o x=R w/o questo
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


def RandomLayerConstant(N=default_N, **kwargs):
    """Return a class ResidualGaussianMultihead non-trainable
    with a random unitary (a constant RandomLayer)

    Parameters
    ----------
    N: size of the vector

    Returns
    -------
    a Constant RandomLayer
    """

    return RandomLayer(
        N, trainable_M=False, trainable_d=False, trainable=False, **kwargs
    )


class RandomIdentityLayer(layers.Layer):
    """Define a linear random layer corresponding to the
    a = U a
    with U a unitary complex matrix
    In the R space, corresponds to
    x = R x
    In input has input N, the size of the R vector (must be even)

    At variance with RandomLayer the Layer is initialized as an identity

    Parameters in the constructor
    ----------------------------
    :param N: dimension
    :param trainable_M: if false M is not trained, default is true
    :param M_np: covariance matrix of the Gaussian layer
    :param d_np: displacement vector of the Gaussian layer
    (None is default for a zero non trainable d)
    :param trainable_d: if False the Gaussian covariance matrix and displacement are not trainable (defaul is true)

    :output x M
    :output b = M^(-1) d

    """

    def __init__(self, N=10, trainable_M=True, d_np=None, trainable_d=True, **kwargs):
        super(RandomIdentityLayer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension must be even "
        self.trainable_d = trainable_d
        self.trainable_M = trainable_M
        self.N = N
        n = np.floor_divide(self.N, 2)
        # Init the random operator as a unitary operator
        wr_np = np.zeros((n, n), dtype=np_real)
        wi_np = np.zeros((n, n), dtype=np_real)
        # define the name as scope (useful for plotting the model)
        with tf.name_scope(self.name) as scope:
            self.WR = tf.Variable(
                wr_np, dtype=self.dtype, trainable=self.trainable_M, name="WR"
            )
            self.WI = tf.Variable(
                wi_np, dtype=self.dtype, trainable=self.trainable_M, name="WI"
            )
        Rx, Rp, J = RQRP(N)
        self.Rx = tf.constant(Rx, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        self.J = tf.constant(J, dtype=self.dtype)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # the layer has a trainable displacement
            # if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)

    def get_M(self):
        """return the M matrix and its inverse MI"""
        # generate symmetric matrix
        HR = self.WR + tf.transpose(self.WR)
        # generate an antisymmetric matrix
        HI = self.WI - tf.transpose(self.WI)
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # return the real and immaginary part
        UR = tf.math.real(U)
        UI = tf.math.imag(U)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI

    def call(self, x, di=None):
        # generate symmetric matrix
        HR = self.WR + tf.transpose(self.WR)
        # generate an antisymmetric matrix
        HI = self.WI - tf.transpose(self.WI)
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # return the real and immaginary part
        UR = tf.math.real(U)
        UI = tf.math.imag(U)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        # no transpose here for M, as M is already transpose
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        # return tensor list(check this ... and test x,y =R o x=R w/o questo
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


class meanRLayer(layers.Layer):
    """Return the expectation value of operator R
    as derivative
    of the imaginary part of the model to and evaluated at x=0

    In the constructor:

    Parameters:
    -----------
    N: size of the of vector

    Returns in the call:
    -------------------
    The mean value of R at x=0

    Remark
    ------
    In the call we also include chir and chii, even if as
    they are not used, this is needed to TensorFlow to
    to create the Graph

    """

    def __init__(self, N, **kwargs):
        super(meanRLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # x=0 variable
        self.x0 = tf.constant(np.zeros((1, self.N)), self.dtype)

    def call(self, chir, chii, pullback):
        """Return the derivative at x=0
        Parameters:
        -----------
        chir (needed but dummy)
        chii (needed but dummy)
        pullback : model to be derived"""
        with tf.GradientTape() as tape:
            tape.watch(self.x0)
            _, ci = pullback(self.x0)
            chii_x = tape.gradient(ci, self.x0)
        return chii_x


class CovarianceLayer(layers.Layer):
    """
    Return the covariance of a characteristic function
    from the derivatives of the model

    Given
    x as 1xN vector
    d as Nx1 vector
    g as NxN symmetric matrix

    For a Gaussian model one has

    chi(x)=exp(-0.25 x^T g x+i x d)=chi_R+ i chi_I

    d = gradient(chi_I(x)) at x=0

    g(m,n) = -2.0 d^2 chi_R / ( dx_m dx_n)-2.0 (d chi_I / dx_m)( d chi_I/dx_n)
    with all derivatives evaluate at x=0 and m,n =0,1, ..., N-1

    Hence the matrix g is obtained by the Hessian of the model,
    which can be calculated by the tf.gradient.jacobian function

    In the constructor:

    Parameters
    ----------
    N: dimension

    In the call

    Parameters
    ----------
    c1: real part of chi
    c2: imag part of chi
    param: pullback: model to be derived wrt x

    Returns
    -------
    cov: covariance matrix g (N,N)
    ci_x: expected R (=transpose of d) (1,N)
    Hessian: Hessian at x =0 (N,N)
    """

    def __init__(self, N, **kwargs):
        super(CovarianceLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # constant x0 (not to be trained, x=0 for the derivatives)
        self.x0 = tf.constant(np.zeros((1, self.N)), dtype=self.dtype)

    @tf.function
    def call(self, c1, c2, chi):
        # actually c1 and c2 are dummy here, but are needed to make a model with this layer
        x = self.x0  # x=0 (must be constant as non trainable)
        with tf.GradientTape() as t1:
            t1.watch(x)  # watch constant
            with tf.GradientTape(
                persistent=True
            ) as t2:  # t2 is persistent as used two times
                t2.watch(x)
                cr, ci = chi(x)
            cr_x = t2.gradient(cr, x)
            ci_x = t2.gradient(ci, x)
        d2cr = t1.jacobian(cr_x, x)  # hessian
        # reshape the Hessian to a matrix
        Hessian = tf.reshape(d2cr, [self.N, self.N])
        # evaluate the covariance with the factor 2.0 to account for the definition of g
        cov = -2 * (Hessian + tf.matmul(ci_x, ci_x, transpose_a=True))
        # free t2
        del t2
        return cov, ci_x, Hessian


class PhaseModulatorLayer(layers.Layer):
    """Phase Modulator Layer
    Define a multi head linear random layer corresponding to the
    a = U a
    with U a unitary complex matrix with diagonal elements
    [exp(i theta1),exp(i theta2),...,exp(i theta_{N/2})]

    In the space, corresponds to
    y = x M
    In input has input n, the size of the R vector (must be even)

    Parameters
    ----------
    N: dimension
    trainable_M: if false M is not trained, default is true
    M_np: covariance matrix of the Gaussian layer
    d_np: displacement vector of the Gaussian layer (None is default for a zero non trainable d)
    trainable_d: if False the Gaussian covariance matrix and displacement are not trainable (default is true)

    Returns
    -------
    y = x M
    b = M^(-1) d

    """

    def __init__(
        self,
        N=default_N,
        trainable_M=True,
        d_np=None,
        trainable_d=True,
        phases_np=None,
        **kwargs,
    ):
        super(PhaseModulatorLayer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension must be even "
        self.trainable_d = trainable_d
        self.trainable_M = trainable_M
        self.N = N
        n = np.floor_divide(self.N, 2)
        # random generation of phases
        if phases_np is None:
            theta_np = np.random.random((n,))
        else:
            assert (
                phases_np.shape[0] == n
            ), "Input phases must be a vector with shape (n,1)"
            theta_np = phases_np
        self.theta = tf.Variable(theta_np, dtype=self.dtype, trainable=self.trainable_M)
        Rq, Rp, J = RQRP(N)
        self.Rq = tf.constant(Rq)
        self.Rp = tf.constant(Rp)
        self.J = tf.constant(J)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # the layer has a trainable displacement if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)

    def get_M(self):
        """return the M matrix and its inverse MI
        also return UR and UI
        M, MI, UR, UI = layer.get_M()
        """
        # Build the symplectiv matrix M
        UR = tf.linalg.diag(tf.cos(self.theta))
        UI = tf.linalg.diag(tf.sin(self.theta))
        M = (
            tf.matmul(self.Rq, tf.matmul(UR, self.Rq, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rq, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rq, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI, UR, UI

    def call(self, x, di=None):
        # return the real and immaginary part
        UR = tf.linalg.diag(tf.cos(self.theta))
        UI = tf.linalg.diag(tf.sin(self.theta))
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rq, tf.matmul(UR, self.Rq, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rq, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rq, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        # no transpose here for M, as M is already transpose
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


class SingleModeSqueezerLayer(layers.Layer):
    """Single mode squeeze operator

    Define a multi-head linear random layer corresponding to a
    single mode squeeze operator

    In input has input N, the size of the R vector (must be even)

    The parameters r and theta, and the index of the squeezed mode

    nsqueezing goes from 0 to (N/2)-1  // index of the squeezed modes

    Parameters
    ----------
    N: dimension
    r_np : squeezing parameter
    theta_np : angle squeezing parameter
    n_squeezed : index of the squeezed mode (from 0 to N/2)
    trainable : if true the parameters r and theta are trainable (default is True)
    d_np: if not empty corresponds to a gate with d' not zero (default is None)
    trainable_d: if true d' is trainable (default is False)

    Returns
    -------
    y = x M
    b = M^(-1) d

    """

    def __init__(
        self,
        N=10,
        r_np=1.0,
        theta_np=0.0,
        n_squeezed=0,
        trainable=True,
        d_np=None,
        trainable_d=False,
        autocast=False,
        **kwargs,
    ):
        super(SingleModeSqueezerLayer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension N must be even "
        assert n_squeezed < N / 2, " Cannot squeeze a mode index bigger than N-1 "
        self.trainable_d = trainable_d
        self.trainable = trainable
        self.N = N
        self.n_squeezed = n_squeezed
        with tf.name_scope(self.name) as scope:
            # squeezing parameters
            self.theta = tf.Variable(
                theta_np, dtype=self.dtype, trainable=self.trainable, name="theta"
            )
            self.r = tf.Variable(
                r_np, dtype=self.dtype, trainable=self.trainable, name="r"
            )
            if d_np is None:
                # the layer has a constant zero displacement
                self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
                trainable_d = False
            else:
                # the layer has a trainable displacement
                # if trainable_d is set to true (default is false)
                self.d = tf.Variable(
                    d_np, dtype=self.dtype, trainable=trainable_d, name="d"
                )
        Rq, Rp, J = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        self.J = tf.constant(J, dtype=self.dtype)
        # padding vector for the matrix
        self.paddings = tf.constant(
            [
                [2 * n_squeezed, self.N - 2 * (n_squeezed + 1)],
                [2 * n_squeezed, self.N - 2 * (n_squeezed + 1)],
            ]
        )

    @tf.function
    def get_M(self):
        # return the M matrix and its inverse MI
        # Build the symplectiv matrix M
        M11 = (
            tf.math.cosh(self.r) - tf.math.cos(self.theta) * tf.math.sinh(self.r) - 1
        )  # substract 1 in the diag elements, as then sum identiy
        M12 = tf.math.sin(-self.theta) * tf.math.sinh(self.r)
        M21 = tf.math.sin(-self.theta) * tf.math.sinh(self.r)
        M22 = tf.math.cosh(self.r) + tf.math.cos(self.theta) * tf.math.sinh(self.r) - 1
        L1 = tf.stack([M11, M12, M21, M22], 0)
        L2 = tf.reshape(L1, (2, 2))
        L3 = tf.pad(L2, self.paddings, constant_values=0)
        M = L3 + tf.eye(self.N, dtype=self.dtype)
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI

    def call(self, x, di=None):
        # build M and its inverse
        M, MI = self.get_M()
        # build
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


class TwoModeSqueezerLayer(layers.Layer):
    """
    Define a multi head linear random layer corresponding to
    two mode squeezing operator

    Paramters:
    ----------
    N: dimension
    r_np : squeezing parameter
    theta_np : angle squeezing parameter
    n_s1 : index of the squeezed mode 1 (from 0 to N/2)
    n_s2 : index of the squeezed mode 2 (from 0 to N/2)
    trainable : if true the parameters r and theta are trainable (default is True)
    d_np: if not empty corresponds to a gate with d' not zero (default is None)
    trainable_d: if true d' is trainable (default is False)


    Returns:
    --------
    y = x M
    b = M^(-1) d

    """

    def __init__(
        self,
        N=default_N,
        r_np=1.0,
        theta_np=0.0,
        n_s1=0,
        n_s2=1,
        trainable=True,
        d_np=None,
        trainable_d=False,
        **kwargs,
    ):
        super(TwoModeSqueezerLayer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension N must be even "
        assert N > 3, " Dimension N must be greater than 3 "
        assert n_s1 < N / 2, " Cannot squeeze a mode index bigger than N-1, n_s1<N/2 "
        assert n_s2 < N / 2, " Cannot squeeze a mode index bigger than N-1, n_s2<N/2 "
        assert (
            n_s2 != n_s1
        ), " Two mode squeezing operator needs different modes n_s1 and n_s2 "
        self.trainable_d = trainable_d
        self.trainable = trainable
        self.N = N
        self.n_s1 = n_s1
        self.n_s2 = n_s2
        # squeezing parameters
        self.theta = tf.Variable(theta_np, dtype=self.dtype, trainable=self.trainable)
        self.r = tf.Variable(r_np, dtype=self.dtype, trainable=self.trainable)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # the layer has a trainable displacement if trainable_d is true
            # (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)
        Rq, Rp, J = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        self.J = tf.constant(J, dtype=self.dtype)
        # padding vector for the matrix
        self.paddings11 = tf.constant(
            [[2 * n_s1, self.N - 2 * (n_s1 + 1)], [2 * n_s1, self.N - 2 * (n_s1 + 1)]]
        )
        self.paddings12 = tf.constant(
            [[2 * n_s1, self.N - 2 * (n_s1 + 1)], [2 * n_s2, self.N - 2 * (n_s2 + 1)]]
        )
        self.paddings21 = tf.constant(
            [[2 * n_s2, self.N - 2 * (n_s2 + 1)], [2 * n_s1, self.N - 2 * (n_s1 + 1)]]
        )
        self.paddings22 = tf.constant(
            [[2 * n_s2, self.N - 2 * (n_s2 + 1)], [2 * n_s2, self.N - 2 * (n_s2 + 1)]]
        )

    @tf.function
    def get_M(self):
        # return the M matrix and its inverse MI
        # Build the symplectiv matrix M
        M11 = (
            tf.math.cosh(self.r) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M12 = tf.constant(0.0, dtype=self.dtype)
        M13 = tf.math.sinh(-self.r) * tf.math.cos(self.theta)
        M14 = tf.math.sin(self.theta) * tf.math.sinh(-self.r)

        M21 = tf.constant(0.0, dtype=self.dtype)
        M22 = (
            tf.math.cosh(self.r) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M23 = tf.math.sin(self.theta) * tf.math.sinh(-self.r)
        # note minus here in sinh
        M24 = tf.math.cos(self.theta) * tf.math.sinh(self.r)

        M31 = tf.math.cos(self.theta) * tf.math.sinh(-self.r)
        M32 = tf.math.sin(self.theta) * tf.math.sinh(-self.r)
        M33 = (
            tf.math.cosh(self.r) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M34 = tf.constant(0.0, dtype=self.dtype)

        M41 = tf.math.sin(self.theta) * tf.math.sinh(-self.r)
        # note minus here in sinh
        M42 = tf.math.cos(self.theta) * tf.math.sinh(self.r)
        M43 = tf.constant(0.0, dtype=self.dtype)
        M44 = (
            tf.math.cosh(self.r) - 1
        )  # substract 1 in the diag elements, as then sum identiy

        La = tf.stack([M11, M12, M21, M22], 0)
        L11 = tf.reshape(La, (2, 2))

        Lb = tf.stack([M13, M14, M23, M24], 0)
        L12 = tf.reshape(Lb, (2, 2))

        Lc = tf.stack([M31, M32, M41, M42], 0)
        L21 = tf.reshape(Lc, (2, 2))

        Ld = tf.stack([M33, M34, M43, M44], 0)
        L22 = tf.reshape(Ld, (2, 2))

        Ma = tf.pad(L11, self.paddings11, constant_values=0)
        Mb = tf.pad(L12, self.paddings12, constant_values=0)
        Mc = tf.pad(L21, self.paddings21, constant_values=0)
        Md = tf.pad(L22, self.paddings22, constant_values=0)

        M = Ma + Mb + Mc + Md + tf.eye(self.N, dtype=self.dtype)
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI

    def call(self, x, di=None):
        # build M and its inverse
        M, MI = self.get_M()
        # build
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


class PhotonCountingLayer(layers.Layer):
    """Photon Counting Layer

    Return the expected photon number in each mode
    by using the second derivatives of the model (obsolete version)


    Given
    x as 1xN vector
    d as Nx1 vector
    g as NxX symmetric matrix

    Build the (1xN) vector of second derivatives of chi
    chiR_xx chiR_yy ...

    Multiply by (Rq+Rp)/2 and add 1/2

    In the constructor:
    Parameters
    ----------
    N: size of the of vector

    In the call
    Parameters
    ----------
    c1: real part of chi
    c2: imag part of chi
    pullback: model to be derived wrt x

    Returns
    -------
    nboson: vector of expected values of the photon number

    """

    def __init__(self, N, **kwargs):
        super(PhotonCountingLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)

    @tf.function
    def call(self, c1, c2, chi):
        # c1 and c2 are dummy here, needed for a model
        # chi is the model
        # x = tf.Variable(np.zeros((1, self.N)), dtype=tf_real)
        x = tf.zeros([1, self.N], dtype=chi.dtype)
        with tf.GradientTape() as t1:
            t1.watch(x)
            with tf.GradientTape() as t2:
                t2.watch(x)
                cr, _ = chi(x)
            cr_x = t2.gradient(cr, x)
        cr_xx = t1.jacobian(cr_x, x)
        # extract the diagonal part
        tmp1 = tf.reshape(cr_xx, [self.N, self.N])
        lapls = tf.reshape(tf.linalg.diag_part(tmp1), [1, self.N])
        # sum the derivatives to have the photon number
        nqq = tf.matmul(lapls, self.Rq)
        npp = tf.matmul(lapls, self.Rp)
        nboson = -0.5 * (nqq + npp) - 0.5
        return nboson


class BeamSplitterLayer(layers.Layer):
    """Beam Splitter Layer

    Define a layer corresponding to a beam splitter in a many mode state

    Parameters
    ----------
    N: dimension
    n_0 : index of the mode 0 (from 0 to N/2)
    n_1 : index of the mode 1 (from 0 to N/2)
    theta_np : angle parameter (default pi/4 corresponding to 50:50)
    phi0_np : delay parameter (default 0)
    phi1_np : delay parameter (default 0)
    trainable_theta : if true theta trainable (default is True)
    trainable_phi0 : if true phi0 trainable (default is True)
    trainable_phi1 : if true phi1 trainable (default is True)
    d_np: if not empty corresponds to a gate with d' not zero (default is None)
    trainable_d: if true d' is trainable (default is False)

    Returns
    -------
    y = x M
    b = M^(-1) d

    """

    def __init__(
        self,
        N=default_N,
        n_0=0,
        n_1=1,
        theta=np.pi / 4,
        phi0=0.0,
        phi1=0.0,
        trainable_theta=True,
        trainable_phi0=True,
        trainable_phi1=True,
        d_np=None,
        trainable_d=False,
        autocast=False,
        **kwargs,
    ):
        super(BeamSplitterLayer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension N must be even "
        assert N > 3, " Dimension N must be greater than 3 "
        assert n_0 < N / 2, " Cannot squeeze a mode index bigger than N-1, n_0<N/2 "
        assert n_1 < N / 2, " Cannot squeeze a mode index bigger than N-1, n_1<N/2 "
        assert (
            n_1 != n_0
        ), " Two mode squeezing operator needs different modes n_0 and n_1 "
        self.trainable_d = trainable_d
        self.N = N
        self.n_0 = n_0
        self.n_1 = n_1
        # squeezing parameters
        self.theta = tf.Variable(theta, dtype=self.dtype, trainable=trainable_theta)
        self.phi0 = tf.Variable(phi0, dtype=self.dtype, trainable=trainable_phi0)
        self.phi1 = tf.Variable(phi1, dtype=self.dtype, trainable=trainable_phi1)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # trainable displacement if trainable_d is true (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)
        Rq, Rp, J = RQRP(N)
        self.Rq = tf.constant(Rq)
        self.Rp = tf.constant(Rp)
        self.J = tf.constant(J)
        # padding vector for the matrix
        self.paddings11 = tf.constant(
            [[2 * n_0, self.N - 2 * (n_0 + 1)], [2 * n_0, self.N - 2 * (n_0 + 1)]]
        )
        self.paddings12 = tf.constant(
            [[2 * n_0, self.N - 2 * (n_0 + 1)], [2 * n_1, self.N - 2 * (n_1 + 1)]]
        )
        self.paddings21 = tf.constant(
            [[2 * n_1, self.N - 2 * (n_1 + 1)], [2 * n_0, self.N - 2 * (n_0 + 1)]]
        )
        self.paddings22 = tf.constant(
            [[2 * n_1, self.N - 2 * (n_1 + 1)], [2 * n_1, self.N - 2 * (n_1 + 1)]]
        )

    @tf.function
    def get_M(self):
        # return the M matrix and its inverse MI
        # Build the symplectiv matrix M
        # fix the following elements by correct expressions
        M11 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M12 = tf.math.sin(self.phi0) * tf.math.cos(self.theta)
        M13 = tf.math.cos(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin
        M14 = tf.math.sin(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin

        M21 = tf.math.sin(-self.phi0) * tf.math.cos(self.theta)  # note -1 in sin
        M22 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M23 = tf.math.sin(self.phi1) * tf.math.sin(self.theta)
        M24 = tf.math.cos(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin

        M31 = tf.math.cos(self.phi1) * tf.math.sin(self.theta)
        M32 = tf.math.sin(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin
        M33 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M34 = tf.math.sin(-self.phi0) * tf.math.cos(self.theta)  # note -1 in sin

        M41 = tf.math.sin(self.phi1) * tf.math.sin(self.theta)
        M42 = tf.math.cos(self.phi1) * tf.math.sin(self.theta)
        M43 = tf.math.sin(self.phi0) * tf.math.cos(self.theta)  # note -1 in sin
        M44 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identiy

        La = tf.stack([M11, M12, M21, M22], 0)
        L11 = tf.reshape(La, (2, 2))

        Lb = tf.stack([M13, M14, M23, M24], 0)
        L12 = tf.reshape(Lb, (2, 2))

        Lc = tf.stack([M31, M32, M41, M42], 0)
        L21 = tf.reshape(Lc, (2, 2))

        Ld = tf.stack([M33, M34, M43, M44], 0)
        L22 = tf.reshape(Ld, (2, 2))

        Ma = tf.pad(L11, self.paddings11, constant_values=0)
        Mb = tf.pad(L12, self.paddings12, constant_values=0)
        Mc = tf.pad(L21, self.paddings21, constant_values=0)
        Md = tf.pad(L22, self.paddings22, constant_values=0)

        M = Ma + Mb + Mc + Md + tf.eye(self.N)
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI

    def call(self, x, di=None):
        # build M and its inverse
        # return the M matrix and its inverse MI
        # Build the symplectiv matrix M
        # fix the following elements by correct expressions
        M11 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M12 = tf.math.sin(self.phi0) * tf.math.cos(self.theta)
        M13 = tf.math.cos(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin
        M14 = tf.math.sin(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin

        M21 = tf.math.sin(-self.phi0) * tf.math.cos(self.theta)  # note -1 in sin
        M22 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M23 = tf.math.sin(self.phi1) * tf.math.sin(self.theta)
        M24 = tf.math.cos(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin

        M31 = tf.math.cos(self.phi1) * tf.math.sin(self.theta)
        M32 = tf.math.sin(self.phi1) * tf.math.sin(-self.theta)  # note -1 in sin
        M33 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identity
        M34 = tf.math.sin(-self.phi0) * tf.math.cos(self.theta)  # note -1 in sin

        M41 = tf.math.sin(self.phi1) * tf.math.sin(self.theta)
        M42 = tf.math.cos(self.phi1) * tf.math.sin(self.theta)
        M43 = tf.math.sin(self.phi0) * tf.math.cos(self.theta)  # note -1 in sin
        M44 = (
            tf.math.cos(self.phi0) * tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identiy

        La = tf.stack([M11, M12, M21, M22], 0)
        L11 = tf.reshape(La, (2, 2))

        Lb = tf.stack([M13, M14, M23, M24], 0)
        L12 = tf.reshape(Lb, (2, 2))

        Lc = tf.stack([M31, M32, M41, M42], 0)
        L21 = tf.reshape(Lc, (2, 2))

        Ld = tf.stack([M33, M34, M43, M44], 0)
        L22 = tf.reshape(Ld, (2, 2))

        Ma = tf.pad(L11, self.paddings11, constant_values=0)
        Mb = tf.pad(L12, self.paddings12, constant_values=0)
        Mc = tf.pad(L21, self.paddings21, constant_values=0)
        Md = tf.pad(L22, self.paddings22, constant_values=0)

        M = Ma + Mb + Mc + Md + tf.eye(self.N)
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        #        M, MI=self.get_M()
        # build
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


class LaplacianLayer(layers.Layer):
    """Laplacian per mode at x=0

     The output is the vector

     laplacian_j = (d_{q_j}^2+d_{p_j}^2) chir at x=0

     Given
     x as 1xN vector
     d as Nx1 vector
     g as NxX symmetric matrix

    In the constructor,

    Parameters
    ----------
    N: size of the of vector

     In the call,

     Parameters
     ----------
     c1: real part of chi (dummy)
     c2: imag part of chi (dummy)
     pullback: model to be derived wrt x

     Returns
     -------
     laplacian : vector of laplacian (1,n)
    """

    def __init__(self, N, **kwargs):
        super(LaplacianLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        # constant tensor of indices to extract the diag derivatives
        list1 = []
        for i in range(N):
            list1.append([0, i] * 2)
        # list1 is [[0,0,0,0],[0,1,0,1],[0,2,0,2],[0,3,0,3]] for N=4
        self.indices_diag = tf.constant(list1)
        # constant x0 (not to be trained, x=0 for the derivatives)
        self.x0 = tf.constant(np.zeros((1, self.N)), dtype=self.dtype)

    def call(self, c1, c2, chi):
        # c1 and c2 dummy needed for the model
        # chi is the model
        x = self.x0
        with tf.GradientTape() as t1:
            t1.watch(x)
            with tf.GradientTape() as t2:
                t2.watch(x)
                cr, _ = chi(x)
            cr_x = t2.gradient(cr, x)  # first the gradient shape [1,N]
        cr_xy = t1.jacobian(cr_x, x)  # then the jacobian (shape [1,N,1,N])
        # gather the diagonal part and reshape to matrix multiplication
        cr_xx = tf.reshape(tf.gather_nd(cr_xy, self.indices_diag), [1, self.N])
        # sum the derivatives to have
        dqq = tf.matmul(cr_xx, self.Rq)
        dpp = tf.matmul(cr_xx, self.Rp)
        lapl = dqq + dpp
        return lapl


class BiharmonicLayer(layers.Layer):
    """Biharmonic operator (laplacian squared) per mode at x=0

     The output is the vector

     squaredlaplacian_j = ((d_{q_j}^2+d_{p_j}^2))^2 chir at x=0

     Given
     x as 1xN vector
     d as Nx1 vector
     g as NxX symmetric matrix

    In the constructor,

    Parameters
    ----------
     N: size of the of vector

     In the call,

    Parameters
    -----------
     c1: real part of chi
     c2: imag part of chi
     pullback: model to be derived wrt x

    Returns
    --------
     biharmonic : vector of laplacian squared (n,1)
     laplacian : vector of laplacian (n,1)
    """

    def __init__(self, N, **kwargs):
        super(BiharmonicLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # mode number
        self.n = tf.constant(np.floor_divide(N, 2))
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        # indeces to diagonal the biharmonic and laplacian
        list_2x = []
        for i in range(N):
            list_2x.append([0, i] * 2)
        # list_2x is [[0,0,0,0],[0,1,0,1],[0,2,0,2],[0,3,0,3]] for N=4
        self.indices_2x = tf.constant(list_2x)
        # constant x0 (not to be trained, x=0 for the derivatives)
        self.x0 = tf.constant(np.zeros((1, self.N)), dtype=self.dtype)

    @tf.function
    def call(self, c1, c2, chi):
        # c1 and c2 dummy needed to make a model
        # chi is the model
        x = self.x0
        with tf.GradientTape() as t4:
            t4.watch(x)
            with tf.GradientTape() as t3:
                t3.watch(x)
                with tf.GradientTape() as t2:
                    t2.watch(x)
                    with tf.GradientTape() as t1:
                        t1.watch(x)
                        cr, _ = chi(x)  # shape [1,1]
                    cr_x = t1.gradient(cr, x)  # shape [1,N]
                cr_xy = t2.jacobian(cr_x, x)  # shape [1,N,1,N]
            cr_xyz = t3.jacobian(cr_xy, x)  # shape [1,N,1,N,1,N]
        cr_xyzw = t4.jacobian(cr_xyz, x)  # shape [1,N,1,N,1,N,1,N]
        cr_xyzw = tf.reshape(
            cr_xyzw, [self.N, self.N, self.N, self.N]
        )  # shape [N,N,N,N]
        # extract the diagonal part by gather and shape as [1,N]
        cr_2x = tf.reshape(tf.gather_nd(cr_xy, self.indices_2x), [1, self.N])
        # sum the derivatives to have the squared laplacian
        dqq = tf.matmul(cr_2x, self.Rq)
        dpp = tf.matmul(cr_2x, self.Rp)
        laplacian = dqq + dpp  # [1,N]

        # build the biharmonic
        biharmonic = tf.zeros_like(laplacian)  # [1,N]
        for j in tf.range(self.n):
            qj = 2 * j
            pj = 2 * j + 1
            dqqpp = tf.gather_nd(cr_xyzw, [[qj, qj, pj, pj]])
            dqqqq = tf.gather_nd(cr_xyzw, [[qj, qj, qj, qj]])
            dpppp = tf.gather_nd(cr_xyzw, [[pj, pj, pj, pj]])
            biharmonic = tf.tensor_scatter_nd_add(
                biharmonic, [[0, j]], dqqqq + dpppp + 2 * dqqpp
            )

        return biharmonic, laplacian


class HeisenbergLayer(layers.Layer):
    """Return n, n2, Dn2

    Use the BiharmonicLayer

    In the constructor:
    Parameters
    ----------
    N: size of the of vector

    In the call,

    Parameters
    ----------
    c1: real part of chi (dummy)
    c2: imag part of chi (dummy)
    pullback: model to be derived wrt x

    Returns
    -------
    nboson: expected number of bosons per mode (1,n)
    n2: expected squared nboson  per mode (1,n)
    Dn2: gexpected values of squared uncertainties (1,n)
    """

    def __init__(self, N, **kwargs):
        super(HeisenbergLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # BiharmonicLayer
        self.Biharmonic = BiharmonicLayer(N, **kwargs)

    def call(self, c1, c2, chi):
        # c1 and c2 are dummy as in LaplacianLayer
        # chi is the model

        # call the biharmonic layer
        lapl2, lapl = self.Biharmonic(c1, c2, chi)

        # evaluate the number of bosons
        nboson = -0.5 * lapl - 0.5

        # evaluate the n2
        nboson2 = 0.25 * lapl2 + 0.5 * lapl

        # evaluate the uncertainties
        Dnboson2 = 0.25 * lapl2 - 0.25 * tf.square(lapl) - 0.25

        return nboson, nboson2, Dnboson2


class BiharmonicGaussianLayer(layers.Layer):
    """Biharmonic operator (laplacian squared) at x=0 for Gaussian state

     The output is the vector

     squaredlaplacian_j = ((d_{q_j}^2+d_{p_j}^2))^2 chir at x=0

     and is computed by using the covariance matrix and
     the displacement vector

    In the constructor,
    Parameters
    ----------
    N: size of the of vector

    In the call,

    Parameters
    ----------
    c1: real part of chi
    c2: imag part of chi
    pullback: model to be derived wrt x

    Returns
    -------
    biharmonic : vector of laplacian squared (n,1)
    laplacian : vector of laplacian (n,1)
    """

    def __init__(self, N, **kwargs):
        super(BiharmonicGaussianLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # mode number
        self.n = tf.constant(np.floor_divide(N, 2))
        # covariance layer
        self.covariance = CovarianceLayer(N, **kwargs)
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)

    @tf.function
    def call(self, c1, c2, chi, **kwargs):
        # c1 and c2 are dummy here, but are needed to make a model with this layer
        # chi is the model

        # evaluate the covariace and the displacement
        g, d, hessian = self.covariance(c1, c2, chi)

        # evaluate the laplacian by gathering the diagonal elements of gaussian
        cr_2x = tf.reshape(tf.linalg.diag_part(hessian), [1, self.N])

        # sum the derivatives to have the squared laplacian
        dqq = tf.matmul(cr_2x, self.Rq)  # [1,N]
        dpp = tf.matmul(cr_2x, self.Rp)  # [1,N]
        laplacian = dqq + dpp  # [1,N]

        # evaluate the biharmonic by diag g and d
        d = tf.squeeze(d)  # [N,] dj
        gd = tf.linalg.diag_part(g)  # [N,] gjj
        d2 = tf.square(d)  # [N,] dj^2
        d4 = tf.tensordot(d2, d2, axes=0)  # [N,N] dj^2 dk^2
        dd = tf.tensordot(d, d, axes=0)  # [N,N] dj dk

        djjkk = (
            0.25 * tf.tensordot(gd, gd, axes=0)
            + 0.5 * tf.square(g)
            + 0.5 * tf.tensordot(gd, d2, axes=0)
            + 0.5 * tf.tensordot(d2, gd, axes=0)
            + 2.0 * tf.multiply(g, dd)
            + d4
        )

        biharmonic = tf.zeros_like(laplacian)  # [1,N]
        for j in tf.range(self.n):
            qj = 2 * j
            pj = 2 * j + 1
            dqqpp = tf.gather_nd(djjkk, [[qj, pj]])
            dqqqq = tf.gather_nd(djjkk, [[qj, qj]])
            dpppp = tf.gather_nd(djjkk, [[pj, pj]])
            biharmonic = tf.tensor_scatter_nd_add(
                biharmonic, [[0, j]], dqqqq + dpppp + 2 * dqqpp
            )

        return biharmonic, laplacian


class HeisenbergGaussianLayer(layers.Layer):
    """Return n, n2, Dn2 for Gaussian states

    Use the BiharmonicGaussianLayer

    In the constructor,
    Parameters
    ----------
    N: size of the of vector

    In the call,

    Parameters
    ----------
    c1: real part of chi (dummy)
    c2: imag part of chi (dummy)
    pullback: model to be derived wrt x

    Returns
    -------
    nboson: expected number of bosons per mode (1,n)
    n2: expected squared nboson  per mode (1,n)
    Dn2: expected values of squared uncertainties (1,n)
    """

    def __init__(self, N, **kwargs):
        super(HeisenbergGaussianLayer, self).__init__(**kwargs)
        # vector size
        self.N = N
        # BiharmonicLayer
        self.Biharmonic = BiharmonicGaussianLayer(N, **kwargs)

    def call(self, c1, c2, chi, **kwargs):
        # c1 and c2 are dummy as in LaplacianLayer
        # chi is the model

        # call the biharmonic Gaussian layer
        lapl2, lapl = self.Biharmonic(c1, c2, chi)

        # evaluate the number of bosons
        nboson = -0.5 * lapl - 0.5

        # evaluate the n2
        nboson2 = 0.25 * lapl2 + 0.5 * lapl

        # evaluate the uncertainties
        Dnboson2 = 0.25 * lapl2 - 0.25 * tf.square(lapl) - 0.25

        return nboson, nboson2, Dnboson2


class DifferentialGaussianLayer(layers.Layer):
    """Returns mean nj-nk and (nk-nk)^2 for a Gaussian model

     Use the biharmonic Gaussian layer

    In the constructor,

    Parameters
    ----------
    N: size of the of vector

     In the call,

     Parameters
     ----------
     c1: real part of chi
     c2: imag part of chi
     pullback: model to be derived wrt x

     Returns
     -------
     nboson : (1,n) tensor with <nj>
     dn : (n,n) tensor with <nj-nk>
     dn2 : (n,n) tensor with <(nj-nk)^2>

    """

    def __init__(self, N, **kwargs):
        super(DifferentialGaussianLayer, self).__init__(**kwargs)
        # vector size
        self.N = tf.constant(N)
        self.n = tf.constant(np.floor_divide(N, 2))
        # covariance layer
        self.covariance = CovarianceLayer(N, **kwargs)
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        # tensors of mixed derivatives
        self.cxx = tf.zeros([1, N], dtype=self.dtype)
        self.cxxyy = tf.zeros([N, N], dtype=self.dtype)

    @tf.function
    def build_differences(self, nb1):
        # build a tensor with the differences
        #
        # param: nb1: tensor with shape (1,n)
        #
        # Returns
        # param: dn: tensor nj-nk with shape (n,n)

        dn = tf.zeros([self.n, self.n], dtype=self.dtype)
        for j in tf.range(self.n):
            nj = tf.gather_nd(nb1, [[0, j]])
            for k in tf.range(j + 1, self.n):
                nk = tf.gather_nd(nb1, [[0, k]])
                indices = [[j, k]]
                dn = tf.tensor_scatter_nd_add(dn, indices, nj - nk)
                indices = [[k, j]]
                dn = tf.tensor_scatter_nd_add(dn, indices, -nj + nk)

        return dn

    def call(self, c1, c2, chi):
        # c1 and c2 dummy but needed for the model
        # chi is the model

        # evaluate the covariace and the displacement
        g, d, hessian = self.covariance(c1, c2, chi)

        # evaluate the laplacian by gathering the diagonal elements of gaussian
        cr_2x = tf.reshape(tf.linalg.diag_part(hessian), [1, self.N])

        # sum the derivatives to have the squared laplacian
        dqq = tf.matmul(cr_2x, self.Rq)
        dpp = tf.matmul(cr_2x, self.Rp)
        lapl = dqq + dpp

        # evaluate the number of bosons
        nboson = -0.5 * lapl - 0.5

        # tensor with differences
        dn = self.build_differences(nboson)

        # # evaluate biharmonic
        gdiag = tf.linalg.diag_part(g)
        d2 = tf.square(d)
        d4 = tf.square(d2)

        # build nabla2 nabla2
        d2 = tf.squeeze(d2)
        d4 = tf.squeeze(d4)

        # build tensor gjk^2 [N,N]
        gjkjk = tf.square(g)

        # build tensor gjj gkk [N,N]
        gjjkk = tf.tensordot(gdiag, gdiag, axes=0)

        # build tensor gjj dk dk [N,N]
        gjjdkdk = tf.tensordot(gdiag, d2, axes=0)

        # build tensor gkk dj dj [N,N]
        gkkdjdj = tf.tensordot(d2, gdiag, axes=0)

        # build tensor gjk dj dk [N,N]
        dd = tf.squeeze(tf.tensordot(d, d, axes=0))  # [N,N]
        gjkdjdk = tf.multiply(g, dd)

        # build tensor dj dj dk dk [N,N]
        djdjdkdk = tf.tensordot(d2, d2, axes=0)

        # this is a NxN tensor [N,N]
        cr_xxyy = (
            0.5 * gjkjk
            + 0.25 * gjjkk
            + 0.5 * gjjdkdk
            + 2.0 * gjkdjdk
            + 0.5 * gkkdjdj
            + djdjdkdk
        )  # make a double loop to build the nablas

        dn2 = tf.zeros([self.n, self.n], dtype=self.dtype)
        # the following double loop can be optimized to be symmetric
        for j in tf.range(self.n):
            qj = 2 * j
            pj = 2 * j + 1
            # nabla^4 j
            d_qj4 = tf.gather_nd(cr_xxyy, [[qj, qj]])  # d_qjqjqjqj
            d_pj4 = tf.gather_nd(cr_xxyy, [[pj, pj]])  # d_qjqjqjqj
            d_pj2qj2 = tf.gather_nd(cr_xxyy, [[qj, pj]])  # d_pjpjqjqj
            #            d_qj2 = tf.gather_nd(cr_2x, [[0, qj]])  # d_qjqj
            #            d_pj2 = tf.gather_nd(cr_2x, [[0, pj]])  # d_pjpj
            n4j = d_qj4 + d_pj4 + 2 * d_pj2qj2  # 2 * d_qj2 * d_pj2  # nabla^4_j
            for k in tf.range(j + 1, self.n):
                qk = 2 * k
                pk = 2 * k + 1
                # nabla^4 k
                d_qk4 = tf.gather_nd(cr_xxyy, [[qk, qk]])  # d_qkqkqkqk
                d_pk4 = tf.gather_nd(cr_xxyy, [[pk, pk]])  # d_pkpkpkpk
                d_pk2qk2 = tf.gather_nd(cr_xxyy, [[qk, pk]])  # d_pkpkqkqk
                #                d_qk2 = tf.gather_nd(cr_2x, [[0, qk]])  # d_qjqj
                #                d_pk2 = tf.gather_nd(cr_2x, [[0, pk]])  # d_pjpj
                n4k = d_qk4 + d_pk4 + 2 * d_pk2qk2  # 2 * d_qk2 * d_pk2  # nabla^4 k

                # nabla^2_j nabla^2_k
                n2j2k = (
                    tf.gather_nd(cr_xxyy, [[qj, qk]])
                    + tf.gather_nd(cr_xxyy, [[qj, pk]])
                    + tf.gather_nd(cr_xxyy, [[qk, pj]])
                    + tf.gather_nd(cr_xxyy, [[pk, pj]])
                )
                # store the final value
                njnk = 0.25 * (n4j + n4k) - 0.5 * n2j2k - 0.5
                indices = [[j, k]]
                dn2 = tf.tensor_scatter_nd_add(dn2, indices, njnk)
                indices = [[k, j]]
                dn2 = tf.tensor_scatter_nd_add(dn2, indices, njnk)

        return nboson, dn, dn2


class DifferentialGaussianLayerBKP(layers.Layer):
    """Returns mean nj-nk and (nk-nk)^2 for a Gaussian model

     Use the biharmonic Gaussian layer

    In the constructor,

    Parameters
    ----------
    N: size of the of vector

     In the call,

     Parameters
     ----------
     c1: real part of chi
     c2: imag part of chi
     pullback: model to be derived wrt x

     Returns
     -------
     nboson : (1,n) tensor with <nj>
     dn : (n,n) tensor with <nj-nk>
     dn2 : (n,n) tensor with <(nj-nk)^2>

    """

    def __init__(self, N, **kwargs):
        super(DifferentialGaussianLayer, self).__init__(**kwargs)
        # vector size
        self.N = tf.constant(N)
        self.n = tf.constant(np.floor_divide(N, 2))
        # covariance layer
        self.covariance = CovarianceLayer(N, **kwargs)
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        # tensors of mixed derivatives
        self.cxx = tf.zeros([1, N], dtype=self.dtype)
        self.cxxyy = tf.zeros([N, N], dtype=self.dtype)

    @tf.function
    def build_differences(self, nb1):
        # build a tensor with the differences
        #
        # param: nb1: tensor with shape (1,n)
        #
        # Returns
        # param: dn: tensor nj-nk with shape (n,n)

        dn = tf.zeros([self.n, self.n], dtype=self.dtype)
        for j in tf.range(self.n):
            nj = tf.gather_nd(nb1, [[0, j]])
            for k in tf.range(j + 1, self.n):
                nk = tf.gather_nd(nb1, [[0, k]])
                indices = [[j, k]]
                dn = tf.tensor_scatter_nd_add(dn, indices, nj - nk)
                indices = [[k, j]]
                dn = tf.tensor_scatter_nd_add(dn, indices, -nj + nk)

        return dn

    def call(self, c1, c2, chi):
        # c1 and c2 dummy but needed for the model
        # chi is the model

        # evaluate the covariace and the displacement
        g, d, hessian = self.covariance(c1, c2, chi)

        # evaluate the laplacian by gathering the diagonal elements of gaussian
        cr_2x = tf.reshape(tf.linalg.diag_part(hessian), [1, self.N])

        # sum the derivatives to have the squared laplacian
        dqq = tf.matmul(cr_2x, self.Rq)
        dpp = tf.matmul(cr_2x, self.Rp)
        lapl = dqq + dpp

        # evaluate the number of bosons
        nboson = -0.5 * lapl - 0.5

        # tensor with differences
        dn = self.build_differences(nboson)

        # # evaluate biharmonic
        gdiag = tf.linalg.diag_part(g)
        d2 = tf.square(d)
        d4 = tf.square(d2)

        # build nabla2 nabla2
        d2 = tf.squeeze(d2)
        d4 = tf.squeeze(d4)

        # build tensor gjk^2 [N,N]
        gjkjk = tf.square(g)

        # build tensor gjj gkk [N,N]
        gjjkk = tf.tensordot(gdiag, gdiag, axes=0)

        # build tensor gjj dk dk [N,N]
        gjjdkdk = tf.tensordot(gdiag, d2, axes=0)

        # build tensor gkk dj dj [N,N]
        gkkdjdj = tf.tensordot(d2, gdiag, axes=0)

        # build tensor gjk dj dk [N,N]
        dd = tf.squeeze(tf.tensordot(d, d, axes=0))  # [N,N]
        gjkdjdk = tf.multiply(g, dd)

        # build tensor dj dj dk dk [N,N]
        djdjdkdk = tf.tensordot(d2, d2, axes=0)

        # this is a NxN tensor [N,N]
        cr_xxyy = (
            0.5 * gjkjk
            + 0.25 * gjjkk
            + 0.5 * gjjdkdk
            + 2.0 * gjkdjdk
            + 0.5 * gkkdjdj
            + djdjdkdk
        )  # make a double loop to build the nablas

        dn2 = tf.zeros([self.n, self.n], dtype=self.dtype)
        # the following double loop can be optimized to be symmetric
        for j in tf.range(self.n):
            qj = 2 * j
            pj = 2 * j + 1
            # nabla^4 j
            d_qj4 = tf.gather_nd(cr_xxyy, [[qj, qj]])  # d_qjqjqjqj
            d_pj4 = tf.gather_nd(cr_xxyy, [[pj, pj]])  # d_qjqjqjqj
            d_qj2 = tf.gather_nd(cr_2x, [[0, qj]])  # d_qjqj
            d_pj2 = tf.gather_nd(cr_2x, [[0, pj]])  # d_pjpj
            n4j = d_qj4 + d_pj4 + 2 * d_qj2 * d_pj2  # nabla^4_j
            for k in tf.range(j + 1, self.n):
                qk = 2 * k
                pk = 2 * k + 1
                # nabla^4 k
                d_qk4 = tf.gather_nd(cr_xxyy, [[qk, qk]])  # d_qjqjqjqj
                d_pk4 = tf.gather_nd(cr_xxyy, [[pk, pk]])  # d_qjqjqjqj
                d_qk2 = tf.gather_nd(cr_2x, [[0, qk]])  # d_qjqj
                d_pk2 = tf.gather_nd(cr_2x, [[0, pk]])  # d_pjpj
                n4k = d_qk4 + d_pk4 + 2 * d_qk2 * d_pk2  # nabla^4 k

                # nabla^2_j nabla^2_k
                n2j2k = (
                    tf.gather_nd(cr_xxyy, [[qj, qk]])
                    + tf.gather_nd(cr_xxyy, [[qj, pk]])
                    + tf.gather_nd(cr_xxyy, [[qk, pj]])
                    + tf.gather_nd(cr_xxyy, [[pk, pj]])
                )
                # store the final value
                njnk = 0.25 * (n4j + n4k) - 0.5 * n2j2k - 0.5
                indices = [[j, k]]
                dn2 = tf.tensor_scatter_nd_add(dn2, indices, njnk)
                indices = [[k, j]]
                dn2 = tf.tensor_scatter_nd_add(dn2, indices, njnk)

        return nboson, dn, dn2


class entanglementBSLayer(layers.Layer):
    """
    Return the entanglement of a mode evaluated as the
    the nu parameter and
    E = -ln (2/(delta+1))-((delta-1)/2)ln((delta-1)/(delta+1))
    as for a beam splitter with non classical states

    Take as input the covariance matrix of a model


        In the constructor,
        Parameters
        ----------
        ne: index of the mode to be calculated

        In the call,
        Parameters
        ----------
        cov: covariance matrix of the model
    """

    def __init__(self, N, ne=0, **kwargs):
        super(entanglementBSLayer, self).__init__(autocast=False, **kwargs)
        assert ne < N // 2, " mode index must be smaller thant N//2"
        self.N = N
        self.ne = ne

    def Enuf(self, delta):
        E = -tf.math.log(2 / (delta + 1)) - (delta - 1) * 0.5 * tf.math.log(
            (delta - 1) / (delta + 1)
        )
        return E

    def call(self, cov):
        """
        Return parameter nu, and E
        """
        N0 = cov.shape[0]
        N1 = cov.shape[1]
        assert N0 == N1, " covariance matrix must be square"
        assert N0 == self.N, " covariance matrix must be have sinze N"
        ca = tf.slice(cov, [2 * self.ne, 2 * self.ne], [2, 2])
        delta = 0.5 * tf.math.sqrt(tf.linalg.det(ca))
        Edelta = self.Enuf(delta)
        return delta, Edelta, ca


def QTransform(g, d):
    # return the Q-transform of a model, starting
    # from the covariance matrix and the displacement vector
    # of the model
    #
    # Parameters
    # ----------
    # g : covariance matrix [N,N]
    # d : displacement vector [1,N]
    #
    # Return a function of the k-vector with shape [1,N]

    N, _ = g.shape
    n = N // 2
    EN = tf.eye(N, dtype=g.dtype)
    Q = tf.multiply(0.5, g + EN)
    QI = tf.linalg.inv(Q)

    scale = tf.math.pow(tf.linalg.det(Q), tf.constant(-0.5))  # check this constant

    def functionQ(kin):
        k = kin - d
        yR = tf.matmul(QI, k, transpose_b=True)
        gDot = tf.matmul(-0.5 * k, yR)
        f = tf.exp(gDot)
        f = tf.multiply(f, tf.exp(tf.matmul(0.5 * kin, kin, transpose_b=True)))
        f = tf.multiply(f, scale)
        return f

    return functionQ


class QTransformLayer(tf.keras.layers.Layer):
    """Return a layer for the Q-transform
    Usage
    -----
    QLayer = QTransformLayer(N, model)
    kin=tf.keras.layers.Input([N],dtype=model.dtype)
    Q = QLayer(kin)
    Qrho = tf.keras.Model(inputs=kin, outputs=Q)
    """

    def __init__(self, N, model):
        super(QTransformLayer, self).__init__()
        self.N = N
        self.EN = tf.eye(self.N, dtype=self.dtype)
        self.model = model

        x0 = tf.zeros([1, N], dtype=model.dtype)
        # covlayer = ps.CovarianceLayer2(N)
        covlayer = CovarianceLayer(N)
        cr, ci = model(x0)
        cov1, mean_R1, _ = covlayer(cr, ci, model)
        self.g = cov1
        self.d = mean_R1
        self.Q = tf.multiply(0.5, self.g + self.EN)
        self.QI = tf.linalg.inv(self.Q)
        self.scale = tf.math.pow(tf.linalg.det(self.Q), tf.constant(-0.5))

    def call(self, kin):
        k = kin - self.d
        yR = tf.matmul(self.QI, k, transpose_b=True)
        gDot = tf.matmul(-0.5 * k, yR)
        f = tf.exp(gDot)
        f = tf.multiply(f, tf.exp(tf.matmul(0.5 * kin, kin, transpose_b=True)))
        f = tf.multiply(f, self.scale)

        return f


def getQTransformModel(model):
    """Return a model with the Qtransform

    Parameters
    ----------
    model: model to be transformed

    Returns
    -------
    The vector kin and the model Qrho corresponding
    to the modified Q-representation of the model
    """
    N = model.input_shape[1]
    QLayer = QTransformLayer(N, model)
    kin = tf.keras.layers.Input([N], dtype=model.dtype)
    Q = QLayer(kin)
    Qrho = tf.keras.Model(inputs=kin, outputs=Q)
    return kin, Qrho


class HaarLayerConstant(layers.Layer):
    """
    Define a multi head linear random layer corresponding to the
    a = U a
    with U a Haar unitary complex matrix
    In the R space, corresponds to
    x = R x
    In input has input n, the size of the R vector (must be even)

    The Haar unitary matrix is generated by the qutip package (qtip.org)


    :param N: dimension
    :param trainable_M: if false M is not trained, default is true
    :param M_np: covariance matrix of the Gaussian layer
    :param d_np: displacement vector of the Gaussian layer (None is default for a zero non trainable d)
    :param trainable_d: if False the Gaussian covariance matrix and displacement are not trainable (defaul is true)

    :output x M
    :output b = M^(-1) d

    """

    def __init__(self, N=10, d_np=None, trainable_d=False, **kwargs):
        super(HaarLayerConstant, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension must be even "
        self.trainable_d = trainable_d
        self.N = N
        n = np.floor_divide(self.N, 2)
        Rx, Rp, J = RQRP(N)
        self.Rx = tf.constant(Rx)
        self.Rp = tf.constant(Rp)
        self.J = tf.constant(J)
        # generate the haar unitary
        U = random_objects.rand_unitary_haar(n)
        self.U = tf.constant(U.full())
        # return the real and immaginary part
        UR = tf.constant(np.real(U.full()), dtype=self.dtype)
        UI = tf.constant(np.imag(U.full()), dtype=self.dtype)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        self.M = M
        self.MI = MI
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # the layer has a trainable displacement if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)

    def get_M(self):
        return self.M, self.MI

    def call(self, x, di=None):
        # no transpose here for M, as M is already transpose
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, self.M), tf.matmul(self.MI, d2 + self.d)]


"""""" """""" """""" """""" """""" """""" """" """


class LinearConstantMultiHead(layers.Layer):
    """
    Define a multi head linear random layer corresponding to the
    a = U a
    with U a unitary complex matrix
    In the R space, corresponds to
    x = R x
    In input has input n, the size of the R vector (must be even)
    RETURN y and MI inverse of M

    :param n: dimension
    :param M_np: covariance matrix of the Gaussian layer, if None use a default random symplectic
    :param d_np: displacement vector of the Gaussian layer
    :param trainable: if False the Gaussian covariance matrix and displacement are not trainable

    :output x M
    :output b = M^(-1) d

    DOES NOT CHECK IF M IS SYMPLECTIC
    TODO, add control on the input M if symplectic
    TODO, check this documentation
    """

    def __init__(self, N=10, M_np=None, d_np=None, trainable_d=False, **kwargs):
        super(LinearConstantMultiHead, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension must be even "
        self.trainable_d = trainable_d
        self.N = N
        # Create the symplectic matrix
        _, _, J = RQRP(N)
        if M_np is None:
            M_np, MI_np, _ = RandomSymplectic(N)
        else:
            assert M_np.shape[0] == M_np.shape[1], " M must be square"
            assert M_np.shape[0] == N, " M must have size n"
            MI_np = np.matmul(J.transpose(), np.matmul(M_np.transpose(), J))

        self.M = tf.constant(M_np, dtype=self.dtype)

        # Create the displacement vector
        if d_np is None:
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
        else:
            with tf.name_scope(self.name) as scope:
                self.d = tf.Variable(
                    d_np, dtype=self.dtype, trainable=trainable_d, name="d"
                )
        # inverse of M
        self.MI = tf.constant(MI_np, dtype=self.dtype)

    def call(self, x, di=None):
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, self.M), tf.matmul(self.MI, d2 + self.d)]


class ResidualGaussian(layers.Layer):
    """
    Define a Gaussian state as a residual neural network layer

    n is the size of the state vector x (must be even)

    n/2 is the number of bodies



    """

    def __init__(
        self,
        g=np.eye(10, dtype=np_real),
        d=np.zeros((10, 1), dtype=np_real),
        trainable=True,
        **kwargs,
    ):
        super(ResidualGaussian, self).__init__(**kwargs)
        self.trainable = trainable
        assert g.shape[0] == g.shape[1], "ResidualGaussian, g matrix must be square"
        self.n = g.shape[0]
        assert self.n % 2 == 0, " Dimension must be even "
        assert d.shape[0] == self.n, "ResidualGaussian, d must be same dimension as g"
        self.g_np = g
        self.d_np = d
        self.g = tf.Variable(self.g_np, dtype=self.dtype, trainable=self.trainable)
        self.d = tf.Variable(self.d_np, dtype=self.dtype, trainable=self.trainable)
        self.DotLayer = tf.keras.layers.Dot(axes=1)

    def call(self, x):
        yR = tf.matmul(x, self.g, transpose_b=True)
        gDot = self.DotLayer([x, yR])
        dDot = tf.matmul(x, self.d)
        yr = tf.multiply(-0.25, gDot)
        yi = dDot
        return tf.exp(yr) * tf.cos(yi), tf.exp(yr) * tf.sin(yi)


class TwoBlockInterferometer(layers.Layer):
    """
    Define a linear random layer corresponding to a biparted unitary operator UU
    with block matrix (valid only for an even number of modes) with diagonal U

    UU = [U 0; 0 U]
    with UU with shape [n,n] and U with shape [n/2; n/2 ]

    a = UU a
    with U a unitary complex matrix
    Default U is a linear random

    In the R space, corresponds to
    x = R x
    In input has input N, the size of the R vector (must be even)

    Parameter
    ---------
    :param N: dimension
    :param trainable_M: if false M is not trained, default is true
    :param M_np: covariance matrix of the Gaussian layer
    :param d_np: displacement vector of the Gaussian layer (None is default for a zero non trainable d)
    :param trainable_d: if False the Gaussian covariance matrix and displacement are not trainable (defaul is true)

    Returns
    -------
    :output x M
    :output b = M^(-1) d

    """

    def __init__(self, N=10, trainable_M=True, d_np=None, trainable_d=True, **kwargs):
        super(TwoBlockInterferometer, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension must be even "
        self.trainable_d = trainable_d
        self.trainable_M = trainable_M
        self.N = N
        self.n = int(np.floor_divide(N, 2))
        n = np.floor_divide(self.N, 2)
        assert n % 2 == 0, " Number of modes must be even "
        halfn = int(np.floor_divide(self.n, 2))
        self.halfn = halfn
        wr_np = np.random.random((halfn, halfn))
        wi_np = np.random.random((halfn, halfn))
        self.WR = tf.Variable(wr_np, dtype=self.dtype, trainable=self.trainable_M)
        self.WI = tf.Variable(wi_np, dtype=self.dtype, trainable=self.trainable_M)
        Rx, Rp, J = RQRP(N)
        self.Rx = tf.constant(Rx, dtype=self.dtype)
        self.Rp = tf.constant(Rp, dtype=self.dtype)
        self.J = tf.constant(J, dtype=self.dtype)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=self.dtype)
            trainable_d = False
        else:
            # the layer has a trainable displacement if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=self.dtype, trainable=trainable_d)

    def get_M(self):
        # generate symmetric matrix
        HR = self.WR + tf.transpose(self.WR)
        # generate an antisymmetric matrix
        HI = self.WI - tf.transpose(self.WI)
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # build the block matrix
        Ehn = tf.zeros([self.halfn, self.halfn], dtype=U.dtype)
        UU1 = tf.concat([U, Ehn], 0)
        UU2 = tf.concat([Ehn, U], 0)
        UU = tf.concat([UU1, UU2], 1)
        # return the real and immaginary part
        UR = tf.math.real(UU)
        UI = tf.math.imag(UU)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI, U, UU

    def call(self, x, di=None):
        # generate symmetric matrix
        HR = self.WR + tf.transpose(self.WR)
        # generate an antisymmetric matrix
        HI = self.WI - tf.transpose(self.WI)
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # build the block matrix
        Ehn = tf.zeros([self.halfn, self.halfn], dtype=U.dtype)
        UU1 = tf.concat([U, Ehn], 0)
        UU2 = tf.concat([Ehn, U], 0)
        UU = tf.concat([UU1, UU2], 1)
        # return the real and immaginary part
        UR = tf.math.real(UU)
        UI = tf.math.imag(UU)
        # Build the symplectic matrix M
        M = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        # no traspose here for M, as M is already transpose
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=self.dtype)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


def ConstantRandomLayer(N, **kwargs):
    """Return an nontrainable object LinearRandomMultihead
    The random matrix is generated in the layer
    param: N : size
    """
    return LinearRandomMultiHead(N, trainable_M=False, **kwargs)


# def RandomLayer(N, **kwargs):
#     """Return a trainable object LinearRandomMultihead
#     The random matrix is generated in the layer
#     param: N : size
#     """
#     return LinearRandomMultiHead(N, trainable_M=True, **kwargs)


def TrainableDisplacementLayer(dtarget_np, **kwargs):
    """Return a object LinearConstantMultihead non-trainable with identity matrix
    and input displacement
    param: dtarget: displacement vector
    output: LinearConstantMultihead layer with N=dtarget.shape()[-1] M=np.eye(N) and d=dtarget trainable=False
    """
    N = dtarget_np.shape[0]
    assert dtarget_np.shape[-1] == 1, " Dtarget must be a column vector"
    return LinearConstantMultiHead(
        N, np.eye(N, dtype=np_real), dtarget_np, trainable_d=True, **kwargs
    )


def GlauberLayer(dtarget_np, **kwargs):
    """Return a object LinearConstantMultihead non-trainable with identity matrix
    and input displacement

    Same as DisplacementLayer

    param: dtarget: displacement vector
    output: LinearConstantMultihead layer with N=dtarget.shape()[-1] M=np.eye(N) and d=dtarget trainable=False
    """
    N = dtarget_np.shape[0]
    assert dtarget_np.shape[-1] == 1, " Dtarget must be a column vector"
    return LinearConstantMultiHead(
        N, np.eye(N), dtarget_np, trainable_d=False, **kwargs
    )


class avgR(layers.Layer):
    """Return the expectation value of operator R
    as derivative
    of the imaginary part of the model to and evaluated at x=0

    In the constructor:
    param: n: size of the of vector

    In the call
    param: c1: real part of chi
    param: c2: imag part of chi
    param: pullback: model to be derived wrt x
    """

    def __init__(self, N, **kwargs):
        super(avgR, self).__init__(**kwargs)
        # vector size
        self.N = N
        # x=0 variable
        self.x0 = tf.constant(np.zeros((1, self.N)))

    def call(self, c1, c2, pullback):
        with tf.GradientTape() as tape:
            tape.watch(self.x0)
            _, ci = pullback(self.x0)
            chii_x = tape.gradient(ci, self.x0)
        return chii_x


def covariance(N=default_N, **kwargs):
    """Return a CovarianceLayer, for backward compatibility
    param: N: size of the vector x
    output: CovarianceLayer(N)
    """

    return CovarianceLayer(N, **kwargs)


class CovarianceLayer2(layers.Layer):
    """
    Return the covariance of a characteristic function
    from the derivatives of the model

    Given
    x as 1xN vector
    d as Nx1 vector
    g as NxN symmetric matrix

    For a Gaussian model one has

    chi(x)=exp(-0.25 x^T g x+i x d)=chi_R+ i chi_I

    d = gradient(chi_I(x)) at x=0

    g(m,n) = -2.0 d^2 chi_R / ( dx_m dx_n)-2.0 (d chi_I / dx_m)( d chi_I/dx_n)
    with all derivatives evaluate at x=0 and m,n =0,1, ..., N-1

    Hence the matrix g is obtained by the Hessian of the model,
    which can be calculated by the tf.gradient.jacobian function

    In the constructor:
    param: n: size of the of vector

    In the call
    param: c1: real part of chi
    param: c2: imag part of chi
    param: pullback: model to be derived wrt x

    Returns
    -------
    param: cov: covariance matrix g (N,N)
    param: ci_x: expected R (=transpose of d) (1,N)
    param: Hessian: Hessian at x =0 (N,N)
    """

    def __init__(self, N, **kwargs):
        super(CovarianceLayer2, self).__init__(**kwargs)
        # vector size
        self.N = N
        # constant x0 (not to be trained, x=0 for the derivatives)
        self.x0 = tf.constant(np.zeros((1, self.N)), dtype=self.dtype)

    @tf.function
    def call(self, chi):
        # actually c1 and c2 are dummy here, but are needed to make a model with this layer
        x = self.x0  # x=0 (must be constant as non trainable)
        with tf.GradientTape() as t1:
            t1.watch(x)  # watch constant
            with tf.GradientTape(
                persistent=True
            ) as t2:  # t2 is persistent as used two times
                t2.watch(x)
                cr, ci = chi(x)
            cr_x = t2.gradient(cr, x)
            ci_x = t2.gradient(ci, x)
        d2cr = t1.jacobian(cr_x, x)  # hessian
        # reshape the Hessian to a matrix
        Hessian = tf.reshape(d2cr, [self.N, self.N])
        # evaluate the covariance with the factor 2.0 to account for the definition of g
        cov = -2 * (Hessian + tf.matmul(ci_x, ci_x, transpose_a=True))
        # free t2
        del t2
        return cov, ci_x, Hessian


class e2(layers.Layer):
    """
    this may be canceled as not used
    """

    def __init__(self, N, ne=0, **kwargs):
        super(e2, self).__init__(**kwargs)
        # vector size
        self.N = N
        self.ne = ne
        # x=0 variable
        self.x0 = tf.constant(np.zeros((1, self.N)))

    def Enuf(self, nu_in):
        nu = nu_in
        E = tf.math.log(2 / (nu + 1)) - (nu - 1) * 0.5 * tf.math.log(
            (nu - 1) / (nu - 1)
        )
        return E

    def call(self, c1, c2, chi):
        # actually c1 and c2 are dummy here, but are needed to make a model with this layer
        x = self.x0
        with tf.GradientTape(persistent=True) as t1:
            t1.watch(x)
            with tf.GradientTape(persistent=True) as t2:
                t2.watch(x)
                cr, ci = chi(x)
            cr_x = t2.gradient(cr, x)
            ci_x = t2.gradient(ci, x)
        d2cr = t1.jacobian(cr_x, x)
        # reshape the Jacobian to a matrix
        Hc2 = tf.reshape(d2cr, [self.N, self.N])
        # evaluate the covariance with the factor 2.0 to account for the definition of g
        cov = -2 * (Hc2 + tf.matmul(ci_x, ci_x, transpose_a=True))
        ca = tf.slice(cov, [2 * self.ne, 2 * self.ne], [2, 2])
        nu = tf.linalg.det(ca)
        Enu = self.Enuf(nu)
        del t1
        del t2
        return nu, Enu, ca, cov, ci_x


class PhotonCounterLayerOld(layers.Layer):
    """
    Return the expected photon number in each mode
    by using the second derivatives of the model (obsolete version)


    Given
    x as 1xN vector
    d as Nx1 vector
    g as NxX symmetric matrix

    Build the (1xN) vector of second derivatives of chi
    chiR_xx chiR_yy ...

    Multiply by (Rq+Rp)/2 and add 1/2

    In the constructor:
    param: N: size of the of vector

    In the call
    param: c1: real part of chi
    param: c2: imag part of chi
    param: pullback: model to be derived wrt x

    Returns
    param: n: vector of expected values of the photon number
    """

    def __init__(self, N, **kwargs):
        super(PhotonCounterLayerOld, self).__init__(**kwargs)
        # vector size
        self.N = N
        # x=0 variable
        self.x0 = tf.constant(np.zeros((1, self.N)), dtype=tf.float32)
        # constant matrix to extract the modes
        Rq, Rp, _ = RQRP(N)
        self.Rq = tf.constant(Rq)
        self.Rp = tf.constant(Rp)

    def call(self, c1, c2, chi):
        # c1 and c2 are dummy here, but are needed to make a model with this layer
        # chi is the model
        x = self.x0
        with tf.GradientTape(persistent=True) as t1:
            t1.watch(x)
            with tf.GradientTape(persistent=True) as t2:
                t2.watch(x)
                cr, _ = chi(x)
            cr_x = t2.gradient(cr, x)
            # ci_x = t2.gradient(ci, x)
        # d2cr= t1.jacobian(cr_x,x)
        # d2ci= t1.jacobian(ci_x,x)
        d2cr_xx = t1.gradient(cr_x, x)
        # d2ci_xx=t1.gradient(ci_x,x)
        # reshape the Jacobian to a matrix
        # H2cr = tf.reshape(d2cr, [self.N,self.N])
        # H2ci = tf.reshape(d2ci, [self.N,self.N])
        # clear variables
        del t1
        del t2
        # sum the derivatives to have the photon number
        nq = tf.matmul(d2cr_xx, self.Rq)
        np = tf.matmul(d2cr_xx, self.Rp)
        nboson = -0.5 * (nq + np) - 0.5
        return nboson


@tf.function
def GaussianBosonNumbers(tensor, **kwargs):
    """Function that returns n,n2, and Dn2 given g,d,hessian
    (used in variational ansatz)

    In the call

    param: tensor[0]: covariance matrix (N,N)
    param: tensor[1]: avg displacement vector (1,N)
    param: tensor[2]: hessian matrix (N,N)
    param: tensor[3]: matrix Rq (N,N)
    param: tensor[4]: matrix Rp (N,N)

    g=tensor[0]
    d=tensor[1]
    hessian=tensor[2]
    Rq=tensor[3] # these are passed to speed up
    Rp=tensor[4]

        Returns
        param: nboson: number of bosons per mode (1,n)
        param: nboson2: squared number of bosons per mode (1,n)

    """
    g = tensor[0]
    d = tensor[1]
    hessian = tensor[2]
    Rq = tensor[3]
    Rp = tensor[4]
    N = g.shape[0]
    n = np.floor_divide(N, 2)

    # evaluate the laplacian by gathering the diagonal elements of gaussian
    cr_2x = tf.reshape(tf.linalg.diag_part(hessian), [1, N])

    # sum the derivatives to have the squared laplacian
    dqq = tf.matmul(cr_2x, Rq)  # [1,N]
    dpp = tf.matmul(cr_2x, Rp)  # [1,N]
    laplacian = dqq + dpp  # [1,N]

    # evaluate the biharmonic by diag g and d
    d = tf.squeeze(d)  # [N,] dj
    gd = tf.linalg.diag_part(g)  # [N,] gjj
    d2 = tf.square(d)  # [N,] dj^2
    d4 = tf.tensordot(d2, d2, axes=0)  # [N,N] dj^2 dk^2
    dd = tf.tensordot(d, d, axes=0)  # [N,N] dj dk

    djjkk = (
        0.25 * tf.tensordot(gd, gd, axes=0)
        + 0.5 * tf.square(g)
        + 0.5 * tf.tensordot(gd, d2, axes=0)
        + 0.5 * tf.tensordot(d2, gd, axes=0)
        + 2.0 * tf.multiply(g, dd)
        + d4
    )

    biharmonic = tf.zeros_like(laplacian)  # [1,N]
    for j in tf.range(n):
        qj = 2 * j
        pj = 2 * j + 1
        dqqpp = tf.gather_nd(djjkk, [[qj, pj]])
        dqqqq = tf.gather_nd(djjkk, [[qj, qj]])
        dpppp = tf.gather_nd(djjkk, [[pj, pj]])
        biharmonic = tf.tensor_scatter_nd_add(
            biharmonic, [[0, j]], dqqqq + dpppp + 2 * dqqpp
        )
    # evaluate the number of bosons
    nboson = -0.5 * laplacian - 0.5

    # evaluate the n2
    nboson2 = 0.25 * biharmonic + 0.5 * laplacian

    return nboson, nboson2


class RiggedRandom(layers.Layer):
    """
    Define a multi head linear random layer corresponding to the
    a = U a
    with U a unitary complex matrix
    for mode 0 it is an identity matrix
    for the other mode is a random symplectic

    In the R space, corresponds to
    x = R x
    In input has input n, the size of the R vector (must be even)

    :param n: dimension
    :param trainable_M: if false M is not trained, default is true
    :param M_np: covariance matrix of the Gaussian layer
    :param d_np: displacement vector of the Gaussian layer (None is default for a zero non trainable d)
    :param trainable_d: if False the Gaussian covariance matrix and displacement are not trainable (defaul is true)

    :output x M
    :output b = M^(-1) d

    """

    def __init__(self, N=10, trainable_M=True, d_np=None, trainable_d=True, **kwargs):
        super(RiggedRandom, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension N must be even (N%2==0)"
        self.trainable_d = trainable_d
        self.trainable_M = trainable_M
        self.N = N
        self.Nr = N - 2  # reduced dimension
        n2 = np.floor_divide(self.N, 2)  # this is n
        assert n2 > 1, " At least two modes are require (n>1) "
        n2r = n2 - 1
        wr_np = np.random.random((n2r, n2r))
        wi_np = np.random.random((n2r, n2r))
        self.WR = tf.Variable(wr_np, dtype=tf_real, trainable=self.trainable_M)
        self.WI = tf.Variable(wi_np, dtype=tf_real, trainable=self.trainable_M)
        Rx, Rp, J = RQRP(self.Nr)
        self.Rx = tf.constant(Rx)
        self.Rp = tf.constant(Rp)
        self.J = tf.constant(J)
        if d_np is None:
            # the layer has a constant (non trainable) zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=tf_real)
            trainable_d = False
        else:
            # the layer has a trainable displacement if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=tf_real, trainable=trainable_d)

    @tf.function
    def get_M(self):
        # return the M matrix and its inverse MI
        # generate symmetric matrix
        HR = tf.multiply(0.5, self.WR + tf.transpose(self.WR))
        # generate an antisymmetric matrix
        HI = tf.multiply(0.5, self.WI - tf.transpose(self.WI))
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # return the real and immaginary part
        UR = tf.math.real(U)
        UI = tf.math.imag(U)
        # Build the symplectiv matrix M
        Mr = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of M
        MIr = tf.matmul(tf.matmul(Mr, self.J), self.J, transpose_a=True)
        # rigg the Mr matrix
        M = self.rigg(Mr)
        # rigg the MI matrix
        MI = self.rigg(MIr)

        return M, MI

    @tf.function
    def rigg(self, Mr):
        # return an enlarged M matrix in the 0 mode
        nr = Mr.shape[0]
        nc = Mr.shape[1]
        assert nr == self.Nr, "rigg_M, matrix must have size N-2"
        assert nc == self.Nr, "rigg_M, matrix must have size N-2"
        # subtract the indentity matrix of size Nr
        M1 = Mr - tf.eye(self.Nr)
        # pad to size N x N
        paddings = tf.constant([[2, 0], [2, 0]])
        M2 = tf.pad(M1, paddings, mode="CONSTANT", constant_values=0)
        # add the matrix eye
        M3 = M2 + tf.eye(self.N)
        return M3

    def call(self, x, di=None):
        # generate symmetric matrix
        HR = tf.multiply(0.5, self.WR + tf.transpose(self.WR))
        # generate an antisymmetric matrix
        HI = tf.multiply(0.5, self.WI - tf.transpose(self.WI))
        # exponentiate the Hermitian matrix time the imaginary unit
        U = tf.linalg.expm(tf.complex(-HI, HR))
        # return the real and immaginary part
        UR = tf.math.real(U)
        UI = tf.math.imag(U)
        # Build the symplectiv matrix M
        Mr = (
            tf.matmul(self.Rx, tf.matmul(UR, self.Rx, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UR, self.Rp, transpose_b=True))
            - tf.matmul(self.Rx, tf.matmul(UI, self.Rp, transpose_b=True))
            + tf.matmul(self.Rp, tf.matmul(UI, self.Rx, transpose_b=True))
        )
        # Inverse of Mr
        MIr = tf.matmul(tf.matmul(Mr, self.J), self.J, transpose_a=True)
        # rigg the Mr matrix
        M = self.rigg(Mr)
        # rigg the MI matrix
        MI = self.rigg(MIr)
        # no traspose here for M, as M is already transpose
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=tf_real)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


class SingleModePhaseModulator(layers.Layer):
    """
    Define a multi head linear random layer corresponding to a
    single mode Phase Modulator

    In input has input N, the size of the R vector (must be even)

    The parameters r and theta, and the index of the squeezed mode

    n_pm goes from 0 to (N/2)-1  // index of the phase modulated mode

    :param N: dimension
    :param theta_np : angle phase modulator
    :param n_pm : index of the phase modulated mode (from 0 to N/2)
    :param trainable : if true the parameter theta is trainable (default is True)
    :param d_np: if not empty corresponds to a gate with d' not zero (default is None)
    :param trainable_d: if true d' is trainable (default is False)

    :output x M
    :output b = M^(-1) d

    """

    def __init__(
        self,
        N=10,
        theta_np=0.0,
        n_pm=0,
        trainable_M=True,
        d_np=None,
        trainable_d=False,
        **kwargs,
    ):
        super(SingleModePhaseModulator, self).__init__(**kwargs)
        assert N % 2 == 0, " Dimension N must be even "
        assert n_pm < N / 2, " Cannot phase-modulate a mode index bigger than N-1 "
        self.trainable_d = trainable_d
        self.trainable_M = trainable_M
        self.N = N
        self.n_pm = n_pm
        # squeezing parameters
        self.theta = tf.Variable(theta_np, dtype=tf_real, trainable=self.trainable_M)
        if d_np is None:
            # the layer has a constant zero displacement
            self.d = tf.constant(np.zeros((N, 1), dtype=np_real), dtype=tf_real)
            trainable_d = False
        else:
            # the layer has a trainable displacement if trainable_d is set to true (default is false)
            self.d = tf.Variable(d_np, dtype=tf_real, trainable=trainable_d)
        Rq, Rp, J = RQRP(N)
        self.Rq = tf.constant(Rq)
        self.Rp = tf.constant(Rp)
        self.J = tf.constant(J)
        # padding vector for the matrix
        self.paddings = tf.constant(
            [[2 * n_pm, self.N - 2 * (n_pm + 1)], [2 * n_pm, self.N - 2 * (n_pm + 1)]]
        )

    @tf.function
    def get_M(self):
        # return the M matrix and its inverse MI
        # Build the symplectiv matrix M
        M11 = (
            tf.math.cos(self.theta) - 1
        )  # substract 1 in the diag elements, as then sum identiy
        M12 = tf.math.sin(-self.theta)
        M21 = tf.math.sin(self.theta)
        M22 = tf.math.cos(self.theta) - 1
        L1 = tf.stack([M11, M12, M21, M22], 0)
        L2 = tf.reshape(L1, (2, 2))
        L3 = tf.pad(L2, self.paddings, constant_values=0)
        M = L3 + tf.eye(self.N)
        # Inverse of M
        MI = tf.matmul(tf.matmul(M, self.J), self.J, transpose_a=True)
        return M, MI

    def call(self, x, di=None):
        # build M and its inverse
        M, MI = self.get_M()
        # build
        if di is None:
            d2 = tf.constant(np.zeros((self.N, 1), dtype=np_real), dtype=tf_real)
        else:
            d2 = di
        return [tf.matmul(x, M), tf.matmul(MI, d2 + self.d)]


def partial(j, fun, k):
    # return the partial derivative function of  wrt to k[j]
    # as a function of k, the shape of the output is [[1]]
    #
    # param : j : index of component in range(N)
    # param : fun :  function to be derived
    # param : k : input tensor to fun [1,N]
    #
    # return function with shape [1,]
    def L(k):
        with tf.GradientTape() as tape:
            tape.watch(k)
            y = fun(k)
        dy = tf.squeeze(tape.gradient(y, k))
        return tf.gather_nd(dy, [[j]])

    return L


def laplacian(modeindex, fun, k):
    # return the laplacian with respect to mode index
    #
    # param: modeindex : index of mode in range(n)
    # param: fun : scalar function to be derived
    # param: k : input tensor
    j = 2 * modeindex
    dj0 = doublepartial(j, fun, k)
    dj1 = doublepartial(j + 1, fun, k)

    def L(k):
        return dj0(k) + dj1(k)

    return L


def doublepartial(j, fun, k):
    # return the laplacian of fun wrt to k[j]
    #
    # param: j : index of the derived variable
    # param: fun : function to be derivate
    # param: k : input tensor of fun
    #
    # return a scalar with the second derivative
    dy1 = partial(j, fun, k)
    dy2 = partial(j, dy1, k)
    return dy2


def Pr(nbar, Qfun, mydtype=tf_real):
    # Boson sample operator
    # Return the probability of sequence in nbar
    # from the Q-transform of the model in the
    # k-function Qfun
    #
    # param: nbar : a list of n integer, as [2,3]
    # param: Qfun : Q-transform of a model
    #
    # Returns the probability as a scalar tensor
    n = len(nbar)
    N = 2 * n
    x = tf.Variable(tf.zeros([1, N], dtype=mydtype))
    dm = Qfun
    for j in tf.range(n):
        for _ in tf.range(nbar[j]):
            dm = laplacian(j, dm, x)

    # normalization factor
    nbar_sum = tf.reduce_sum(nbar).numpy()
    scale = 1
    for j in range(n):
        nj = nbar[j]
        scale = scale * np.math.factorial(nj)

    scale = scale * np.power(2.0, nbar_sum)

    scale = 1.0 / scale

    out = tf.multiply(tf.constant(scale, dtype=mydtype), dm(x))

    return out


class QTransformLayerOld(tf.keras.layers.Layer):
    """Return a layer for the Q-transform
    Usage
    -----
    QLayer = QTransformLayer(N, model)
    kin=tf.keras.layers.Input([N],dtype=model.dtype)
    Q = QLayer(kin)
    Qrho = tf.keras.Model(inputs=kin, outputs=Q)
    """

    def __init__(self, N, model):
        super(QTransformLayer, self).__init__()
        self.N = N
        self.EN = tf.eye(self.N, dtype=self.dtype)
        self.model = model

        x0 = tf.zeros([1, N], dtype=model.dtype)
        # covlayer = ps.CovarianceLayer2(N)
        covlayer = CovarianceLayer(N)
        cr, ci = model(x0)
        cov1, mean_R1, _ = covlayer(cr, ci, model)
        self.g = cov1
        self.d = mean_R1

    def call(self, kin):
        g = self.g
        d = self.d
        Q = tf.multiply(0.5, g + self.EN)
        QI = tf.linalg.inv(Q)

        scale = tf.math.pow(tf.linalg.det(Q), tf.constant(-0.5))

        k = kin - d
        yR = tf.matmul(QI, k, transpose_b=True)
        gDot = tf.matmul(-0.5 * k, yR)
        f = tf.exp(gDot)
        f = tf.multiply(f, tf.exp(tf.matmul(0.5 * kin, kin, transpose_b=True)))
        f = tf.multiply(f, scale)

        return f
